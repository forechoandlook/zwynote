<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Basic - zwy notes</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../stylesheets/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Basic";
        var mkdocs_page_input_path = "triton/basic.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> zwy notes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">docs</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../guide/">使用指南</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Index</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">healthy</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../healthy/behavior/">Behavior</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">cuda</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/cmd/">PTX/SASS指令快速指南</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/other/">Other</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/profile/">在命令行模式下查看CUDA程序性能</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/tilelang/">tilelang</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">triton</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Basic</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#usage-template">usage template</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#advanced-config">advanced config</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#theory">theory</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#triton-ampere-wmma">乱谈Triton Ampere WMMA</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">algorithm</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/merkle/">Merkle</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/minhash/">Minhash</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/openvocie/">Openvocie</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/simhash/">Simhash</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">daily</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../daily/0327/">0327</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../daily/0328/">0328</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../daily/0331/">0331</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../daily/0401/">0401</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">search</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/basic/">Basic</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/search_engine/">Search engine</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/spelling_correction/">Spelling correction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/theone_search_design/">简化版高效存储方案：平衡功能与复杂度</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/theone_search_design_impl/">常驻服务模式实现 (Python版本)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/tools/">Tools</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">bitcoin</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/basic/">Basic</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/bull_bear/">牛市(bull)和熊市(bear)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/chains/">Chains</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/consensus/">consensus</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/lean_chain/">Lean chain</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft_mint/">Nft mint</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/norms/">Norms</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/pos/">pos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/problem/">区块链问题解决方法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/solana/">Solana</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/story/">基本流程</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/theory/">Theory</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/tps/">tps</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">train</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../train/unsloth/">Unsloth</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">cc</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/cudac/">4090 的cuda特性</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/mix/">Mix</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/smart_point/">Smart point</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/type_erasure/">Type erasure</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/xapian/">Xapian</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/xapian2/">Xapian2</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">bitcoin/nft</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft/qa/">Qa</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft/text_nft/">文本 nft</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft/text_nft_anti/">Text nft anti</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">zwy notes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">triton</li>
      <li class="breadcrumb-item active">Basic</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="toc">
<ul>
<li><a href="#basic">basic</a><ul>
<li><a href="#usage-template">usage template</a><ul>
<li><a href="#advanced-config">advanced config</a></li>
</ul>
</li>
<li><a href="#theory">theory</a><ul>
<li><a href="#triton-ampere-wmma">乱谈Triton Ampere WMMA</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h1 id="basic">basic<a class="headerlink" href="#basic" title="Permanent link">&para;</a></h1>
<p>https://triton-lang.org/main/python-api/triton.language.html</p>
<h2 id="usage-template">usage template<a class="headerlink" href="#usage-template" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">triton</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">triton.language</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tl</span>

<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kernel</span><span class="p">(</span>
    <span class="n">x_ptr</span><span class="p">,</span>  <span class="c1"># *Pointer* to first input vector</span>
    <span class="n">y_ptr</span><span class="p">,</span>  <span class="c1"># *Pointer* to second input vector</span>
    <span class="n">output_ptr</span><span class="p">,</span>  <span class="c1"># *Pointer* to output vector</span>
    <span class="n">n_elements</span><span class="p">,</span>  <span class="c1"># Size of the vector</span>
    <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># Number of elements each program should process</span>
<span class="p">):</span>
    <span class="c1"># There are multiple &#39;program&#39;s processing different data. We identify which program</span>
    <span class="c1"># we are here</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># We use a 1D launch grid so axis is 0</span>
    <span class="c1"># This program will process inputs that are offset from the initial data.</span>
    <span class="c1"># for instance, if you had a vector of length 256 and block_size of 64, the programs</span>
    <span class="c1"># would each access the elements [0:64, 64:128, 128:192, 192:256].</span>
    <span class="c1"># Note that offsets is a list of pointers</span>
    <span class="n">block_start</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">block_start</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
    <span class="c1"># Create a mask to guard memory operations against out-of-bounds accesses</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">n_elements</span>
    <span class="c1"># Load x and y from DRAM, masking out any extra elements in case the input is not a multiple of the block size</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">x_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="c1"># Write x + y back to DRAM</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">output_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>


<span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">META</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">META</span><span class="p">[</span><span class="s1">&#39;n_elements&#39;</span><span class="p">],</span> <span class="n">META</span><span class="p">[</span><span class="s1">&#39;BLOCK_SIZE&#39;</span><span class="p">]),)</span>
<span class="n">kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</code></pre></div>
<h3 id="advanced-config">advanced config<a class="headerlink" href="#advanced-config" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><code>torch.library.triton_op</code>：
- 这是一个装饰器（decorator），用于包装可能调用一个或多个 Triton 内核的函数
- 它帮助将 Triton 内核集成到 PyTorch 的调度系统中</p>
</li>
<li>
<p><code>torch.library.wrap_triton</code>：
- 这是用来直接包装对 Triton 内核的调用
- 它处理 PyTorch 张量和 Triton 内核之间的数据转换</p>
</li>
</ol>
<p>简单来说，这两个 API 的目的是让开发者能够方便地在 PyTorch 中使用 Triton 编写的高性能 GPU 内核。</p>
<p>举个例子：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">triton</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">triton.language</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tl</span>

<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add_kernel</span><span class="p">(</span><span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">output_ptr</span><span class="p">,</span> <span class="n">n_elements</span><span class="p">):</span>
    <span class="c1"># Triton 内核的实现</span>
    <span class="k">pass</span>

<span class="c1"># 使用 triton_op 包装函数</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">triton_op</span>
<span class="k">def</span><span class="w"> </span><span class="nf">my_add</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># 使用 wrap_triton 包装对 Triton 内核的调用</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">wrap_triton</span><span class="p">(</span><span class="n">add_kernel</span><span class="p">)(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span>
        <span class="n">y</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span>
        <span class="n">output</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span>
        <span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<p>这样做的好处是：
1. 更好的性能优化
2. 更容易集成到 PyTorch 的生态系统中
3. 自动处理设备间的数据传输和类型转换</p>
<h2 id="theory">theory<a class="headerlink" href="#theory" title="Permanent link">&para;</a></h2>
<p><img alt="" src="../2025-04-03-00-00-20.png" /></p>
<p><img alt="" src="../2025-04-03-00-02-20.png" />
<img alt="" src="../2025-04-03-17-55-35.png" /></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">make_ttir</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">opt</span><span class="p">):</span>
    <span class="n">pm</span> <span class="o">=</span> <span class="n">ir</span><span class="o">.</span><span class="n">pass_manager</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">context</span><span class="p">)</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">enable_debug</span><span class="p">()</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">add_inliner</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">ttir</span><span class="o">.</span><span class="n">add_rewrite_tensor_pointer</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">ttir</span><span class="o">.</span><span class="n">add_combine</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">add_canonicalizer</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">ttir</span><span class="o">.</span><span class="n">add_reorder_broadcast</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">add_cse</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">add_licm</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">passes</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">add_symbol_dce</span><span class="p">(</span><span class="n">pm</span><span class="p">)</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
</code></pre></div>
<h3 id="triton-ampere-wmma">乱谈Triton Ampere WMMA<a class="headerlink" href="#triton-ampere-wmma" title="Permanent link">&para;</a></h3>
<p>Blocked Layout: </p>
<div class="highlight"><pre><span></span><code>sizePerThread = [1, 8]：每个线程处理数据Size
threadsPerWarp = [8, 4]： warp内线程的布局
warpsPerCTA = [8, 1]：CTA（Block）内warp的布局
order = [1, 0]：按行访问
</code></pre></div>
<div class="highlight"><pre><span></span><code>#blocked = #triton_gpu.blocked&lt;{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}&gt;
#blocked1 = #triton_gpu.blocked&lt;{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}&gt;
#mma = #triton_gpu.mma&lt;{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0], instrShape = [16, 8]}&gt;
#shared = #triton_gpu.shared&lt;{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1], hasLeadingOffset = false}&gt;
#shared1 = #triton_gpu.shared&lt;{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1], hasLeadingOffset = false}&gt;
</code></pre></div>
<div class="highlight"><pre><span></span><code>module {
  func public @matmul_kernel_0d1d2d3d4c5d6c7d8c(
              %arg0: !tt.ptr&lt;f16&gt; {tt.divisibility = 16 : i32}, ...
    %c32_i32 = arith.constant 32 : i32
    %cst = arith.constant dense&lt;32&gt; : tensor&lt;32x32xi32&gt;
    %c32 = arith.constant 32 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant dense&lt;0.000000e+00&gt; : tensor&lt;32x32xf32&gt;
    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor&lt;32xi32&gt;
    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor&lt;32xi32&gt;) -&gt; tensor&lt;32x1xi32&gt;
    %2 = tt.splat %arg3 : (i32) -&gt; tensor&lt;32x1xi32&gt;
    %3 = arith.muli %1, %2 : tensor&lt;32x1xi32&gt;
    ...
    %11 = arith.muli %1, %10 : tensor&lt;32x1xi32&gt;
    %12 = tt.splat %arg1 : (!tt.ptr&lt;f16&gt;) -&gt; tensor&lt;32x1x!tt.ptr&lt;f16&gt;&gt;
    %13 = tt.addptr %12, %11 : tensor&lt;32x1x!tt.ptr&lt;f16&gt;&gt;, tensor&lt;32x1xi32&gt;
    %14 = tt.broadcast %13 : (tensor&lt;32x1x!tt.ptr&lt;f16&gt;&gt;) -&gt; tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;
    %15 = tt.addptr %14, %8 : tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;, tensor&lt;32x32xi32&gt;
    %16:3 = scf.for %arg6 = %c0 to %c1024 step %c32 iter_args(%arg7 = %cst_0, %arg8 = %9, %arg9 = %15) -&gt; (...)
      %24 = tt.load %arg8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor&lt;32x32xf16&gt;
         ; cp.async.cg.shared.global [ %r44 + 0 ], [ %rd11 + 0 ], 0x10, %r45;
         ; cp.async.cg.shared.global [ %r46 + 0 ], [ %rd12 + 0 ], 0x10, %r45;
      %25 = tt.load %arg9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor&lt;32x32xf16&gt;
         ; cp.async.cg.shared.global [ %r48 + 0 ], [ %rd13 + 0 ], 0x10, %r45;
         ; cp.async.cg.shared.global [ %r50 + 0 ], [ %rd14 + 0 ], 0x10, %r45;
      %26 = tt.dot %24, %25, %arg7 {allowTF32 = true} : tensor&lt;32x32xf16&gt; * tensor&lt;32x32xf16&gt; -&gt; tensor&lt;32x32xf32&gt;
         ; ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r236, %r237, %r238, %r239 }, [ %r56 + 0 ];
         ;  …
         ; ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r232, %r233, %r234, %r235 }, [ %r61 + 0 ];
         ;  …
         ; ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r138, %r139, %r140, %r141 }, [ %r120 + 0 ];
         ;  …
         ; ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r121, %r122, %r123, %r124 }, [ %r125 + 0 ];
         ; mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 },   
         ;       { %r236, %r237, %r238, %r239 }, { %r232, %r233 }, { %f65, %f66, %f67, %f68 };
         ; mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f69, %f70, %f71, %f72 }, 
         ;       { %r236, %r237, %r238, %r239 }, { %r234, %r235 }, { %f69, %f70, %f71, %f72 };
         ; mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, 
         ;       { %r138, %r139, %r140, %r141 }, { %r121, %r122 }, { %f65, %f66, %f67, %f68 };
         ; mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f69, %f70, %f71, %f72 }, 
         ;       { %r138, %r139, %r140, %r141 }, { %r123, %r124 }, { %f69, %f70, %f71, %f72 };
      %27 = tt.addptr %arg8, %cst : tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;, tensor&lt;32x32xi32&gt;
      %28 = arith.muli %arg4, %c32_i32 : i32
      %29 = tt.splat %28 : (i32) -&gt; tensor&lt;32x32xi32&gt;
      %30 = tt.addptr %arg9, %29 : tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;, tensor&lt;32x32xi32&gt;
      scf.yield %26, %27, %30 : tensor&lt;32x32xf32&gt;, tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;, tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;
    }
    %17 = tt.splat %arg5 : (i32) -&gt; tensor&lt;32x1xi32&gt;
    %18 = arith.muli %1, %17 : tensor&lt;32x1xi32&gt;
    %19 = tt.splat %arg2 : (!tt.ptr&lt;f16&gt;) -&gt; tensor&lt;32x1x!tt.ptr&lt;f16&gt;&gt;
    %20 = tt.addptr %19, %18 : tensor&lt;32x1x!tt.ptr&lt;f16&gt;&gt;, tensor&lt;32x1xi32&gt;
    %21 = tt.broadcast %20 : (tensor&lt;32x1x!tt.ptr&lt;f16&gt;&gt;) -&gt; tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;
    %22 = tt.addptr %21, %8 : tensor&lt;32x32x!tt.ptr&lt;f16&gt;&gt;, tensor&lt;32x32xi32&gt;
    %23 = arith.truncf %16#0 : tensor&lt;32x32xf32&gt; to tensor&lt;32x32xf16&gt;
    tt.store %22, %23 {cache = 1 : i32, evict = 1 : i32} : tensor&lt;32x32xf16&gt;
       ; st.global.v4.b32 [ %rd32 + 0 ], { %r158, %r159, %r160, %r161 };
    return
  }
}
</code></pre></div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../cuda/tilelang/" class="btn btn-neutral float-left" title="tilelang"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../algorithm/merkle/" class="btn btn-neutral float-right" title="Merkle">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../cuda/tilelang/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../algorithm/merkle/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../javascripts/mathjax.js"></script>
      <script src="https://cdn.jsdelivr.net/gh/polyfill-io/polyfill-dist@3.111.0/polyfill.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="https://unpkg.com/mermaid@10.4.0/dist/mermaid.min.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
