<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>0328 - zwy notes</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../stylesheets/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "0328";
        var mkdocs_page_input_path = "daily/0328.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> zwy notes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">docs</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../guide/">使用指南</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Index</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">healthy</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../healthy/behavior/">Behavior</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">cuda</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/cmd/">PTX/SASS指令快速指南</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/other/">Other</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/profile/">在命令行模式下查看CUDA程序性能</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cuda/tilelang/">tilelang</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">triton</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../triton/basic/">Basic</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">algorithm</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/merkle/">Merkle</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/minhash/">Minhash</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/openvocie/">Openvocie</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../algorithm/simhash/">Simhash</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">daily</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../0327/">0327</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">0328</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#_1">新的策略</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../0331/">0331</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../0401/">0401</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">search</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/basic/">Basic</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/search_engine/">Search engine</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/spelling_correction/">Spelling correction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/theone_search_design/">简化版高效存储方案：平衡功能与复杂度</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/theone_search_design_impl/">常驻服务模式实现 (Python版本)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../search/tools/">Tools</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">bitcoin</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/basic/">Basic</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/bull_bear/">牛市(bull)和熊市(bear)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/chains/">Chains</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/consensus/">consensus</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/lean_chain/">Lean chain</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft_mint/">Nft mint</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/norms/">Norms</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/pos/">pos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/problem/">区块链问题解决方法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/solana/">Solana</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/story/">基本流程</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/theory/">Theory</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/tps/">tps</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">train</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../train/unsloth/">Unsloth</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">cc</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/cudac/">4090 的cuda特性</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/mix/">Mix</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/smart_point/">Smart point</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/type_erasure/">Type erasure</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/xapian/">Xapian</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cc/xapian2/">Xapian2</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">bitcoin/nft</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft/qa/">Qa</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft/text_nft/">文本 nft</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../bitcoin/nft/text_nft_anti/">Text nft anti</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">zwy notes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">daily</li>
      <li class="breadcrumb-item active">0328</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="vae-3d-conv">vae-3d conv<a class="headerlink" href="#vae-3d-conv" title="Permanent link">&para;</a></h2>
<p>Wan-VAE 分块因果3D卷积</p>
<p>from https://zhuanlan.zhihu.com/p/29268015945</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CausalConv3d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Causal 3d convolusion.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache_x</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cache_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cache_x</span> <span class="o">=</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cache_x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">padding</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-=</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>(Pdb) p self.decoder
WanDecoder3d(
  (nonlinearity): SiLU()
  (conv_in): WanCausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (mid_block): WanMidBlock(
    (attentions): ModuleList(
      (0): WanAttentionBlock(
        (norm): WanRMS_norm()
        (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
        (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (resnets): ModuleList(
      (0-1): 2 x WanResidualBlock(
        (nonlinearity): SiLU()
        (norm1): WanRMS_norm()
        (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (norm2): WanRMS_norm()
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (conv_shortcut): Identity()
      )
    )
  )
  (up_blocks): ModuleList(
    (0): WanUpBlock(
      (resnets): ModuleList(
        (0-1): 2 x WanResidualBlock(
          (nonlinearity): SiLU()
          (norm1): WanRMS_norm()
          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (norm2): WanRMS_norm()
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (conv_shortcut): Identity()
        )
      )
      (upsamplers): ModuleList(
        (0): WanResample(
          (resample): Sequential(
            (0): WanUpsample(scale_factor=(2.0, 2.0), mode=&#39;nearest-exact&#39;)
            (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
        )
      )
    )
    (1): WanUpBlock(
      (resnets): ModuleList(
        (0): WanResidualBlock(
          (nonlinearity): SiLU()
          (norm1): WanRMS_norm()
          (conv1): WanCausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (norm2): WanRMS_norm()
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (conv_shortcut): WanCausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        )
        (1): WanResidualBlock(
          (nonlinearity): SiLU()
          (norm1): WanRMS_norm()
          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (norm2): WanRMS_norm()
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (conv_shortcut): Identity()
        )
      )
      (upsamplers): ModuleList(
        (0): WanResample(
          (resample): Sequential(
            (0): WanUpsample(scale_factor=(2.0, 2.0), mode=&#39;nearest-exact&#39;)
            (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
        )
      )
    )
    (2): WanUpBlock(
      (resnets): ModuleList(
        (0-1): 2 x WanResidualBlock(
          (nonlinearity): SiLU()
          (norm1): WanRMS_norm()
          (conv1): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (norm2): WanRMS_norm()
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (conv_shortcut): Identity()
        )
      )
      (upsamplers): ModuleList(
        (0): WanResample(
          (resample): Sequential(
            (0): WanUpsample(scale_factor=(2.0, 2.0), mode=&#39;nearest-exact&#39;)
            (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
    (3): WanUpBlock(
      (resnets): ModuleList(
        (0-1): 2 x WanResidualBlock(
          (nonlinearity): SiLU()
          (norm1): WanRMS_norm()
          (conv1): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (norm2): WanRMS_norm()
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          (conv_shortcut): Identity()
        )
      )
    )
  )
  (norm_out): WanRMS_norm()
  (conv_out): WanCausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))
)
</code></pre></div>
<p>3D因果VAE架构</p>
<ol>
<li>cache卷积输入，流式推理，每次推理一段，显存占用显著减少</li>
<li><code>Wan-VAE</code>可以编码和解码不限制长度的1080P视频，而不会丢失历史时间信息，使其特别适合视频生成任务</li>
</ol>
<p><img alt="1742972441543" src="../image/0328/1742972441543.png" /></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">WanCausalConv3d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A custom 3D causal convolution layer with feature caching support.</span>

<span class="sd">    This layer extends the standard Conv3D layer by ensuring causality in the time dimension and handling feature</span>
<span class="sd">    caching for efficient inference.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to all three sides of the input. Default: 0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Set up causal padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache_x</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cache_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cache_x</span> <span class="o">=</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cache_x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">padding</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-=</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">WanMidBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">non_linearity</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;silu&quot;</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

        <span class="c1"># Create the components</span>
        <span class="n">resnets</span> <span class="o">=</span> <span class="p">[</span><span class="n">WanResidualBlock</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">non_linearity</span><span class="p">)]</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">attentions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">WanAttentionBlock</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
            <span class="n">resnets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">WanResidualBlock</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">non_linearity</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attentions</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">attentions</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resnets</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">resnets</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feat_idx</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># First residual block</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resnets</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">)</span>

        <span class="c1"># Process through attention and residual blocks</span>
        <span class="k">for</span> <span class="n">attn</span><span class="p">,</span> <span class="n">resnet</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">resnets</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="k">if</span> <span class="n">attn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">WanResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A custom residual block module.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_dim (int): Number of input channels.</span>
<span class="sd">        out_dim (int): Number of output channels.</span>
<span class="sd">        dropout (float, optional): Dropout rate for the dropout layer. Default is 0.0.</span>
<span class="sd">        non_linearity (str, optional): Type of non-linearity to use. Default is &quot;silu&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">non_linearity</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;silu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">non_linearity</span><span class="p">)</span>

        <span class="c1"># layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">WanRMS_norm</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">WanCausalConv3d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">WanRMS_norm</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">WanCausalConv3d</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_shortcut</span> <span class="o">=</span> <span class="n">WanCausalConv3d</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">in_dim</span> <span class="o">!=</span> <span class="n">out_dim</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feat_idx</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># Apply shortcut connection</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># First normalization and activation</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">feat_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">cache_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">CACHE_T</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cache_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">][:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cache_x</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">cache_x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_x</span>
            <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Second normalization and activation</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Dropout</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">feat_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">cache_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">CACHE_T</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cache_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">][:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cache_x</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">cache_x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_x</span>
            <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Add residual connection</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">h</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>    <span class="k">def</span><span class="w"> </span><span class="nf">_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">DecoderOutput</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>

        <span class="n">iter_</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_quant_conv</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="nb">breakpoint</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iter_</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_conv_idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># 逐帧去做的处理</span>
                <span class="n">out</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">feat_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_feat_map</span><span class="p">,</span> <span class="n">feat_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_conv_idx</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">feat_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_feat_map</span><span class="p">,</span> <span class="n">feat_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_conv_idx</span><span class="p">)</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">out</span><span class="p">,</span> <span class="n">out_</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">out</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">DecoderOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">WanDecoder3d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">z_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">dim_mult</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
        <span class="n">num_res_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">attn_scales</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">temperal_upsample</span><span class="o">=</span><span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">non_linearity</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;silu&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span> <span class="o">=</span> <span class="n">z_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_mult</span> <span class="o">=</span> <span class="n">dim_mult</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_res_blocks</span> <span class="o">=</span> <span class="n">num_res_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_scales</span> <span class="o">=</span> <span class="n">attn_scales</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperal_upsample</span> <span class="o">=</span> <span class="n">temperal_upsample</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">non_linearity</span><span class="p">)</span>

        <span class="c1"># dimensions</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span> <span class="o">*</span> <span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="p">[</span><span class="n">dim_mult</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">dim_mult</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dim_mult</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># init block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_in</span> <span class="o">=</span> <span class="n">WanCausalConv3d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># middle blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mid_block</span> <span class="o">=</span> <span class="n">WanMidBlock</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">non_linearity</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># upsample blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
            <span class="c1"># residual (+attention) blocks</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span> <span class="o">//</span> <span class="mi">2</span>

            <span class="c1"># Determine if we need upsampling</span>
            <span class="n">upsample_mode</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dim_mult</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">upsample_mode</span> <span class="o">=</span> <span class="s2">&quot;upsample3d&quot;</span> <span class="k">if</span> <span class="n">temperal_upsample</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;upsample2d&quot;</span>

            <span class="c1"># Create and add the upsampling block</span>
            <span class="n">up_block</span> <span class="o">=</span> <span class="n">WanUpBlock</span><span class="p">(</span>
                <span class="n">in_dim</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span>
                <span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span><span class="p">,</span>
                <span class="n">num_res_blocks</span><span class="o">=</span><span class="n">num_res_blocks</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">upsample_mode</span><span class="o">=</span><span class="n">upsample_mode</span><span class="p">,</span>
                <span class="n">non_linearity</span><span class="o">=</span><span class="n">non_linearity</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">up_block</span><span class="p">)</span>

            <span class="c1"># Update scale for next iteration</span>
            <span class="k">if</span> <span class="n">upsample_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">scale</span> <span class="o">*=</span> <span class="mf">2.0</span>

        <span class="c1"># output blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_out</span> <span class="o">=</span> <span class="n">WanRMS_norm</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_out</span> <span class="o">=</span> <span class="n">WanCausalConv3d</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feat_idx</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1">## conv1</span>
        <span class="k">if</span> <span class="n">feat_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">cache_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">CACHE_T</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># cache last frame of last two chunk</span>
                <span class="n">cache_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">][:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cache_x</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">cache_x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_in</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_x</span>
            <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_in</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">## middle</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mid_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">)</span>

        <span class="c1">## upsamples</span>
        <span class="k">for</span> <span class="n">up_block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">up_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">)</span>

        <span class="c1">## head</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">feat_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">cache_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">CACHE_T</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">cache_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># cache last frame of last two chunk</span>
                <span class="n">cache_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">][:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cache_x</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">cache_x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_out</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">feat_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache_x</span>
            <span class="n">feat_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<div class="mermaid">graph LR
    subgraph "时间维度"
        T1[t-2] --&gt; T2[t-1] --&gt; T3[t]
    end

    subgraph "因果卷积"
        T1 --&gt; O[输出t]
        T2 --&gt; O
        T3 --&gt; O
        style O fill:#f96
    end</div>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">WanUpBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_res_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">upsample_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">non_linearity</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;silu&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>

        <span class="c1"># Create layers list</span>
        <span class="n">resnets</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Add residual blocks and attention if needed</span>
        <span class="n">current_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_res_blocks</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">resnets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">WanResidualBlock</span><span class="p">(</span><span class="n">current_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">non_linearity</span><span class="p">))</span>
            <span class="n">current_dim</span> <span class="o">=</span> <span class="n">out_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">resnets</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">resnets</span><span class="p">)</span>

        <span class="c1"># Add upsampling layer if needed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsamplers</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">upsample_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">upsamplers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">WanResample</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">upsample_mode</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feat_idx</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">resnet</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">resnets</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">feat_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsamplers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">feat_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsamplers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">feat_cache</span><span class="p">,</span> <span class="n">feat_idx</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsamplers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p>现在最大的问题就是 <code>feat_cache</code></p>
<p>貌似每一轮都会有一个cache过程？而且在一个 self.decoder 就会几乎打满25个feat_cache位置。</p>
<!--  -->
<p>这里的动态很多.具体内容包括.</p>
<p>他的shape也很奇怪.</p>
<p>paper: https://files.alicdn.com/tpsservice/5c9de1c74de03972b7aa657e5a54756b.pdf</p>
<p><img alt="1742978006254" src="../image/0328/1742978006254.png" /></p>
<p><img alt="1742978090529" src="../image/0328/1742978090529.png" /></p>
<p>整体结构参考：</p>
<p><img alt="1742978610854" src="../image/0328/1742978610854.png" /></p>
<p>第二压缩帧的内容
<div class="highlight"><pre><span></span><code>x.shape:  torch.Size([1, 16, 1, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
feat_cache torch.Size([1, 16, 2, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 1, 0]
</code></pre></div>
第三压缩帧
<div class="highlight"><pre><span></span><code>x.shape:  torch.Size([1, 16, 1, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
feat_cache torch.Size([1, 16, 2, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [0, 0, 0, 0, 2, 0]
after   [0, 0, 0, 0, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [0, 0, 0, 0, 2, 0]
after   [0, 0, 0, 0, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
</code></pre></div></p>
<p>第4帧
<div class="highlight"><pre><span></span><code>x.shape:  torch.Size([1, 16, 1, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
feat_cache torch.Size([1, 16, 2, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [0, 0, 0, 0, 2, 0]
after   [0, 0, 0, 0, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [0, 0, 0, 0, 2, 0]
after   [0, 0, 0, 0, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
</code></pre></div></p>
<p>5
<div class="highlight"><pre><span></span><code>x.shape:  torch.Size([1, 16, 1, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
feat_cache torch.Size([1, 16, 2, 90, 128])
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [0, 0, 0, 0, 2, 0]
after   [0, 0, 0, 0, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [0, 0, 0, 0, 2, 0]
after   [0, 0, 0, 0, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
before  [1, 1, 1, 1, 2, 0]
after   [1, 1, 1, 1, 0, 0]
</code></pre></div></p>
<p>目前发现 第一帧和第二帧的行为与后面都不太一样</p>
<h3 id="_1">新的策略<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<ol>
<li>得到所有的shape 走动态</li>
</ol>
<p>用 torchview 得到所有的结构和shape</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../0327/" class="btn btn-neutral float-left" title="0327"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../0331/" class="btn btn-neutral float-right" title="0331">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../0327/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../0331/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../javascripts/mathjax.js"></script>
      <script src="https://cdn.jsdelivr.net/gh/polyfill-io/polyfill-dist@3.111.0/polyfill.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="https://unpkg.com/mermaid@10.4.0/dist/mermaid.min.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
