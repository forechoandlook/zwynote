{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u7b14\u8bb0\u76ee\u5f55","text":""},{"location":"guide/","title":"\u4f7f\u7528\u6307\u5357","text":""},{"location":"guide/#_2","title":"\u672c\u5730\u5f00\u53d1","text":""},{"location":"guide/#_3","title":"\u73af\u5883\u51c6\u5907","text":"<ol> <li>\u5b89\u88c5 Python 3.x</li> <li>\u5b89\u88c5 MkDocs \u548c Material \u4e3b\u9898\uff1a</li> </ol> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"guide/#_4","title":"\u672c\u5730\u8fd0\u884c","text":"<ol> <li>\u514b\u9686\u4ed3\u5e93\uff1a</li> </ol> <pre><code>git clone &lt;repository-url&gt;\ncd my-notes\n</code></pre> <ol> <li>\u542f\u52a8\u672c\u5730\u670d\u52a1\u5668\uff1a</li> </ol> <pre><code>mkdocs serve\n</code></pre> <ol> <li>\u5728\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee <code>http://127.0.0.1:8000</code> \u67e5\u770b\u6587\u6863</li> </ol>"},{"location":"guide/#_5","title":"\u6587\u6863\u7ed3\u6784","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md      # \u9996\u9875\n\u251c\u2500\u2500 guide.md      # \u4f7f\u7528\u6307\u5357\n\u2514\u2500\u2500 cc/           # C++ \u76f8\u5173\u6587\u6863\n    \u2514\u2500\u2500 mix.md    # Mix \u6a21\u677f\u7f16\u7a0b\n</code></pre>"},{"location":"guide/#_6","title":"\u8d21\u732e\u6307\u5357","text":""},{"location":"guide/#_7","title":"\u6dfb\u52a0\u65b0\u6587\u6863","text":"<ol> <li>\u5728 <code>docs</code> \u76ee\u5f55\u4e0b\u521b\u5efa Markdown \u6587\u4ef6</li> <li>\u5728 <code>mkdocs.yml</code> \u7684 <code>nav</code> \u90e8\u5206\u6dfb\u52a0\u6587\u6863\u94fe\u63a5</li> <li>\u63d0\u4ea4 Pull Request</li> </ol>"},{"location":"guide/#markdown","title":"Markdown \u89c4\u8303","text":"<ul> <li>\u4f7f\u7528 ATX \u98ce\u683c\u6807\u9898\uff08<code>#</code> \u53f7\uff09</li> <li>\u4ee3\u7801\u5757\u6307\u5b9a\u8bed\u8a00\u7c7b\u578b</li> <li>\u9002\u5f53\u4f7f\u7528\u8868\u683c\u3001\u5217\u8868\u7b49 Markdown \u5143\u7d20</li> <li>\u4fdd\u6301\u6587\u6863\u7ed3\u6784\u6e05\u6670</li> </ul>"},{"location":"guide/#_8","title":"\u672c\u5730\u9884\u89c8","text":"<p>\u4fee\u6539\u6587\u6863\u540e\uff0c\u672c\u5730\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u91cd\u65b0\u52a0\u8f7d\uff0c\u5b9e\u65f6\u9884\u89c8\u66f4\u6539\u3002</p>"},{"location":"guide/#_9","title":"\u90e8\u7f72","text":"<p>\u672c\u6587\u6863\u4f7f\u7528 GitHub Actions \u81ea\u52a8\u90e8\u7f72\u5230 GitHub Pages\u3002\u6bcf\u6b21\u63a8\u9001\u5230 main \u5206\u652f\u65f6\u4f1a\u81ea\u52a8\u89e6\u53d1\u90e8\u7f72\u6d41\u7a0b\u3002</p>"},{"location":"cc/mix/","title":"Mix \u6a21\u677f\u7f16\u7a0b","text":""},{"location":"cc/mix/#mix","title":"mix","text":"<p>from : https://zhuanlan.zhihu.com/p/460825741</p> <p>\u7528\u9014\uff1a\u5c06\u82e5\u5e72\u529f\u80fd\u72ec\u7acb\u7684\u7c7b\u901a\u8fc7\u7ee7\u627f\u7684\u65b9\u5f0f\u5b9e\u73b0\u6a21\u5757\u590d\u7528\u7684C++\u6a21\u677f\u7f16\u7a0b\u6280\u5de7</p> <pre><code>template&lt;typename... Mixins&gt;\nclass MixinClass : public Mixins... {\n  public:\n    MixinClass() :  Mixins...() {}\n  // ...\n};\n</code></pre> <p><code>\u5c06\u6a21\u677f\u53c2\u6570\u4f5c\u4e3a\u6d3e\u751f\u7c7b\u7684\u57fa\u7c7b</code></p> <pre><code>template &lt;typename... Mixins&gt;\nclass Point : public Mixins... {\n public:\n  double x, y;\n  Point() : Mixins()..., x(0.0), y(0.0) {}\n  Point(double x, double y) : Mixins()..., x(x), y(y) {}\n};\n\nclass Label {\n public:\n  std::string label;\n  Label() : label(\"\") {}\n};\n\nclass Color {\n public:\n  unsigned char red = 0, green = 0, blue = 0;\n};\n\nusing MyPoint = Point&lt;Label, Color&gt;;\n</code></pre> <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nstruct Number\n{\n    typedef int value_type;\n    int n;\n    void set(int v) { n = v; }\n    int get() const { return n; }\n};\n\ntemplate &lt;typename BASE, typename T = typename BASE::value_type&gt;\nstruct Undoable\n{\n    typedef T value_type;\n    BASE base;\n    T before;\n    void set(T v) { before = base.get(); base.set(v); }\n    void undo() { base.set(before); }\n    T get() const { return base.get(); }\n};\n\ntemplate &lt;typename BASE, typename T = typename BASE::value_type&gt;\nstruct Redoable\n{\n    typedef T value_type;\n    BASE base;\n    T after;\n    void set(T v) { after = v; base.set(v); }\n    void redo() { base.set(after); }\n    T get() const { return base.get(); }\n};\n\ntypedef Redoable&lt; Undoable&lt;Number&gt; &gt; ReUndoableNumber;\n\nint main()\n{\n    ReUndoableNumber mynum;\n    mynum.set(42); mynum.set(84);\n    cout &lt;&lt; mynum.get() &lt;&lt; '\\n';  // 84\n    mynum.undo();\n    cout &lt;&lt; mynum.get() &lt;&lt; '\\n';  // 42\n    mynum.redo();\n    cout &lt;&lt; mynum.get() &lt;&lt; '\\n';  // back to 84\n}\n</code></pre> <p>\u8fd8\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u5b9e\u73b0\u53ef\u8ddf\u8e2a\u7684\u5f02\u5e38,\u4f46\u662f\u6682\u65f6\u4e0dcare\u4e86</p>"},{"location":"cc/type_erasure/","title":"Type erasure","text":"<pre><code>std::any a;             // a is empty\na = 4.3;                // a has value 4.3 of type double\na = 42;                 // a has value 42 of type int\na = std::string{\"hi\"};  // a has value \"hi\" of type std::string\nif (a.type() == typeid(std::string)) {\n  std::string s = std::any_cast&lt;std::string&gt;(a);\n  UseString(s);\n} else if (a.type() == typeid(int)) {\n  UseInt(std::any_cast&lt;int&gt;(a));\n}\n</code></pre> <p>std::any\u53ef\u4ee5\u7528\u6765\u8868\u793a\u4efb\u4f55\u53ef\u62f7\u8d1d\u6784\u9020\u7684\u5355\u503c\u7c7b\u578b,\u5bf9\u7c7b\u578b\u7684\u6570\u636e\u8fdb\u884c\u4e86\u62bd\u8c61\u3002\u9664\u4e86\u5bf9\u7c7b\u578b\u7684\u6570\u636e\u8fdb\u884c\u62bd\u8c61\u5916\uff0c\u4e5f\u53ef\u4ee5\u5bf9\u7c7b\u578b\u7684\u884c\u4e3a\u8fdb\u884c\u62bd\u8c61\uff0c\u4f8b\u5982std::function\u53ef\u4ee5\u7528\u6765\u8868\u793a\u6240\u6709\u7684\u53ef\u88ab\u8c03\u7528\u7684\u5bf9\u8c61\uff1a\u666e\u901a\u51fd\u6570\u3001\u6210\u5458\u51fd\u6570\u3001\u51fd\u6570\u5bf9\u8c61\u3001lambda\u8868\u8fbe\u5f0f\u3002</p> <p>\u5177\u4f53\u5b9e\u73b0\u662f:</p> <pre><code>class Any {\n  // Holds either pointer to a heap object or the contained object itself.\n  union Storage {\n    constexpr Storage() : ptr{nullptr} {}\n\n    // Prevent trivial copies of this type, buffer might hold a non-POD.\n    Storage(const Storage&amp;) = delete;\n    Storage&amp; operator=(const Storage&amp;) = delete;\n\n    void* ptr;\n    std::aligned_storage_t&lt;sizeof(ptr), alignof(void*)&gt; buffer;\n  };\n  Storage storage_;\n};\n</code></pre> <p>\u8fd9\u4e2a\u65b9\u6cd5\u7684\u4f18\u70b9\u662f\uff1a 1. \u5c0f\u5bf9\u8c61\u4f18\u5316\uff1a\u76f4\u63a5\u5b58\u50a8\u5728 buffer \u4e2d\uff08\u5f53\u5bf9\u8c61\u5927\u5c0f\u4e0d\u8d85\u8fc7\u6307\u9488\u5927\u5c0f\u65f6\uff09 2. \u5927\u5bf9\u8c61\u5b58\u50a8\uff1a\u4f7f\u7528 ptr \u6307\u5411\u5806\u4e0a\u5206\u914d\u7684\u5185\u5b58</p> <pre><code>class Any {\n  union Storage { /*...*/ };\n\n  // \u7528\u4e8e\u5b58\u50a8\u7c7b\u578b\u4fe1\u606f\n  const std::type_info* type_info_;\n\n  // \u7c7b\u578b\u64e6\u9664\u7684\u5173\u952e\uff1a\u51fd\u6570\u6307\u9488\u8868\n  struct VTable {\n      void (*destroy)(Storage&amp;);\n      void (*copy)(Storage&amp;, const Storage&amp;);\n      void (*move)(Storage&amp;, Storage&amp;);\n      const std::type_info&amp; (*type)();\n  };\n  const VTable* vtable_;\n\n  Storage storage_;\n};\n</code></pre> <ol> <li>\u5de5\u4f5c\u539f\u7406</li> <li>\u5f53\u5b58\u50a8\u4e00\u4e2a\u65b0\u503c\u65f6\uff1a</li> <li>\u6839\u636e\u503c\u7684\u5927\u5c0f\u9009\u62e9\u5b58\u50a8\u7b56\u7565</li> <li>\u521b\u5efa\u5bf9\u5e94\u7c7b\u578b\u7684 VTable</li> <li>\u4fdd\u5b58\u7c7b\u578b\u4fe1\u606f</li> <li>\u5728 storage_ \u4e2d\u5b58\u50a8\u6570\u636e</li> </ol> <p>\u7c7b\u578b\u64e6\u9664\u7684\u6838\u5fc3\u5728\u4e8e\uff1a</p> <ol> <li>\u7075\u6d3b\u7684\u5b58\u50a8\u7b56\u7565\uff08union\uff09</li> <li>\u865a\u51fd\u6570\u8868\uff08VTable\uff09\u8bb0\u5f55\u7c7b\u578b\u76f8\u5173\u7684\u64cd\u4f5c</li> <li>\u8fd0\u884c\u65f6\u7c7b\u578b\u4fe1\u606f\uff08type_info\uff09</li> </ol>"},{"location":"daily/0328/","title":"0328","text":""},{"location":"daily/0328/#vae-3d-conv","title":"vae-3d conv","text":"<p>Wan-VAE \u5206\u5757\u56e0\u679c3D\u5377\u79ef</p> <p>from https://zhuanlan.zhihu.com/p/29268015945</p> <pre><code>class CausalConv3d(nn.Conv3d):\n    \"\"\"\n    Causal 3d convolusion.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._padding = (self.padding[2], self.padding[2], self.padding[1],\n                         self.padding[1], 2 * self.padding[0], 0)\n        self.padding = (0, 0, 0)\n\n    def forward(self, x, cache_x=None):\n        padding = list(self._padding)\n        if cache_x is not None and self._padding[4] &gt; 0:\n            cache_x = cache_x.to(x.device)\n            x = torch.cat([cache_x, x], dim=2)\n            padding[4] -= cache_x.shape[2]\n        x = F.pad(x, padding)\n\n        return super().forward(x)\n</code></pre> <pre><code>(Pdb) p self.decoder\nWanDecoder3d(\n  (nonlinearity): SiLU()\n  (conv_in): WanCausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n  (mid_block): WanMidBlock(\n    (attentions): ModuleList(\n      (0): WanAttentionBlock(\n        (norm): WanRMS_norm()\n        (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n        (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x WanResidualBlock(\n        (nonlinearity): SiLU()\n        (norm1): WanRMS_norm()\n        (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n        (norm2): WanRMS_norm()\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n        (conv_shortcut): Identity()\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): WanUpBlock(\n      (resnets): ModuleList(\n        (0-1): 2 x WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): WanResample(\n          (resample): Sequential(\n            (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n            (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n          (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n        )\n      )\n    )\n    (1): WanUpBlock(\n      (resnets): ModuleList(\n        (0): WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): WanCausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n        )\n        (1): WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): WanResample(\n          (resample): Sequential(\n            (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n            (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n          (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n        )\n      )\n    )\n    (2): WanUpBlock(\n      (resnets): ModuleList(\n        (0-1): 2 x WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): WanResample(\n          (resample): Sequential(\n            (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n            (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n    )\n    (3): WanUpBlock(\n      (resnets): ModuleList(\n        (0-1): 2 x WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n    )\n  )\n  (norm_out): WanRMS_norm()\n  (conv_out): WanCausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n)\n</code></pre> <p>3D\u56e0\u679cVAE\u67b6\u6784</p> <ol> <li>cache\u5377\u79ef\u8f93\u5165\uff0c\u6d41\u5f0f\u63a8\u7406\uff0c\u6bcf\u6b21\u63a8\u7406\u4e00\u6bb5\uff0c\u663e\u5b58\u5360\u7528\u663e\u8457\u51cf\u5c11</li> <li><code>Wan-VAE</code>\u53ef\u4ee5\u7f16\u7801\u548c\u89e3\u7801\u4e0d\u9650\u5236\u957f\u5ea6\u76841080P\u89c6\u9891\uff0c\u800c\u4e0d\u4f1a\u4e22\u5931\u5386\u53f2\u65f6\u95f4\u4fe1\u606f\uff0c\u4f7f\u5176\u7279\u522b\u9002\u5408\u89c6\u9891\u751f\u6210\u4efb\u52a1</li> </ol> <p></p> <pre><code>class WanCausalConv3d(nn.Conv3d):\n    r\"\"\"\n    A custom 3D causal convolution layer with feature caching support.\n\n    This layer extends the standard Conv3D layer by ensuring causality in the time dimension and handling feature\n    caching for efficient inference.\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): Zero-padding added to all three sides of the input. Default: 0\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Union[int, Tuple[int, int, int]],\n        stride: Union[int, Tuple[int, int, int]] = 1,\n        padding: Union[int, Tuple[int, int, int]] = 0,\n    ) -&gt; None:\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n        )\n\n        # Set up causal padding\n        self._padding = (self.padding[2], self.padding[2], self.padding[1], self.padding[1], 2 * self.padding[0], 0)\n        self.padding = (0, 0, 0)\n\n    def forward(self, x, cache_x=None):\n        padding = list(self._padding)\n        if cache_x is not None and self._padding[4] &gt; 0:\n            cache_x = cache_x.to(x.device)\n            x = torch.cat([cache_x, x], dim=2)\n            padding[4] -= cache_x.shape[2]\n        x = F.pad(x, padding)\n        return super().forward(x)\n</code></pre> <pre><code>class WanMidBlock(nn.Module):\n\n    def __init__(self, dim: int, dropout: float = 0.0, non_linearity: str = \"silu\", num_layers: int = 1):\n        super().__init__()\n        self.dim = dim\n\n        # Create the components\n        resnets = [WanResidualBlock(dim, dim, dropout, non_linearity)]\n        attentions = []\n        for _ in range(num_layers):\n            attentions.append(WanAttentionBlock(dim))\n            resnets.append(WanResidualBlock(dim, dim, dropout, non_linearity))\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n        self.gradient_checkpointing = False\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        # First residual block\n        x = self.resnets[0](x, feat_cache, feat_idx)\n\n        # Process through attention and residual blocks\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            if attn is not None:\n                x = attn(x)\n\n            x = resnet(x, feat_cache, feat_idx)\n\n        return x\n</code></pre> <pre><code>class WanResidualBlock(nn.Module):\n    r\"\"\"\n    A custom residual block module.\n\n    Args:\n        in_dim (int): Number of input channels.\n        out_dim (int): Number of output channels.\n        dropout (float, optional): Dropout rate for the dropout layer. Default is 0.0.\n        non_linearity (str, optional): Type of non-linearity to use. Default is \"silu\".\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim: int,\n        out_dim: int,\n        dropout: float = 0.0,\n        non_linearity: str = \"silu\",\n    ) -&gt; None:\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.nonlinearity = get_activation(non_linearity)\n\n        # layers\n        self.norm1 = WanRMS_norm(in_dim, images=False)\n        self.conv1 = WanCausalConv3d(in_dim, out_dim, 3, padding=1)\n        self.norm2 = WanRMS_norm(out_dim, images=False)\n        self.dropout = nn.Dropout(dropout)\n        self.conv2 = WanCausalConv3d(out_dim, out_dim, 3, padding=1)\n        self.conv_shortcut = WanCausalConv3d(in_dim, out_dim, 1) if in_dim != out_dim else nn.Identity()\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        # Apply shortcut connection\n        h = self.conv_shortcut(x)\n\n        # First normalization and activation\n        x = self.norm1(x)\n        x = self.nonlinearity(x)\n\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n\n            x = self.conv1(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv1(x)\n\n        # Second normalization and activation\n        x = self.norm2(x)\n        x = self.nonlinearity(x)\n\n        # Dropout\n        x = self.dropout(x)\n\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n\n            x = self.conv2(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv2(x)\n\n        # Add residual connection\n        return x + h\n</code></pre> <pre><code>    def _decode(self, z: torch.Tensor, return_dict: bool = True) -&gt; Union[DecoderOutput, torch.Tensor]:\n        self.clear_cache()\n\n        iter_ = z.shape[2]\n        x = self.post_quant_conv(z)\n        breakpoint()\n        for i in range(iter_):\n            self._conv_idx = [0]\n            if i == 0:\n                # \u9010\u5e27\u53bb\u505a\u7684\u5904\u7406\n                out  = self.decoder(x[:, :, i : i + 1, :, :], feat_cache=self._feat_map, feat_idx=self._conv_idx)\n            else:\n                out_ = self.decoder(x[:, :, i : i + 1, :, :], feat_cache=self._feat_map, feat_idx=self._conv_idx)\n                out = torch.cat([out, out_], 2)\n\n        out = torch.clamp(out, min=-1.0, max=1.0)\n        self.clear_cache()\n        if not return_dict:\n            return (out,)\n\n        return DecoderOutput(sample=out)\n</code></pre> <pre><code>class WanDecoder3d(nn.Module):\n    def __init__(\n        self,\n        dim=128,\n        z_dim=4,\n        dim_mult=[1, 2, 4, 4],\n        num_res_blocks=2,\n        attn_scales=[],\n        temperal_upsample=[False, True, True],\n        dropout=0.0,\n        non_linearity: str = \"silu\",\n    ):\n        super().__init__()\n        self.dim = dim\n        self.z_dim = z_dim\n        self.dim_mult = dim_mult\n        self.num_res_blocks = num_res_blocks\n        self.attn_scales = attn_scales\n        self.temperal_upsample = temperal_upsample\n\n        self.nonlinearity = get_activation(non_linearity)\n\n        # dimensions\n        dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n        scale = 1.0 / 2 ** (len(dim_mult) - 2)\n\n        # init block\n        self.conv_in = WanCausalConv3d(z_dim, dims[0], 3, padding=1)\n\n        # middle blocks\n        self.mid_block = WanMidBlock(dims[0], dropout, non_linearity, num_layers=1)\n\n        # upsample blocks\n        self.up_blocks = nn.ModuleList([])\n        for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n            # residual (+attention) blocks\n            if i &gt; 0:\n                in_dim = in_dim // 2\n\n            # Determine if we need upsampling\n            upsample_mode = None\n            if i != len(dim_mult) - 1:\n                upsample_mode = \"upsample3d\" if temperal_upsample[i] else \"upsample2d\"\n\n            # Create and add the upsampling block\n            up_block = WanUpBlock(\n                in_dim=in_dim,\n                out_dim=out_dim,\n                num_res_blocks=num_res_blocks,\n                dropout=dropout,\n                upsample_mode=upsample_mode,\n                non_linearity=non_linearity,\n            )\n            self.up_blocks.append(up_block)\n\n            # Update scale for next iteration\n            if upsample_mode is not None:\n                scale *= 2.0\n\n        # output blocks\n        self.norm_out = WanRMS_norm(out_dim, images=False)\n        self.conv_out = WanCausalConv3d(out_dim, 3, 3, padding=1)\n\n        self.gradient_checkpointing = False\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        ## conv1\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                # cache last frame of last two chunk\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n            x = self.conv_in(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv_in(x)\n\n        ## middle\n        x = self.mid_block(x, feat_cache, feat_idx)\n\n        ## upsamples\n        for up_block in self.up_blocks:\n            x = up_block(x, feat_cache, feat_idx)\n\n        ## head\n        x = self.norm_out(x)\n        x = self.nonlinearity(x)\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                # cache last frame of last two chunk\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n            x = self.conv_out(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv_out(x)\n        return x\n</code></pre> <pre><code>graph LR\n    subgraph \"\u65f6\u95f4\u7ef4\u5ea6\"\n        T1[t-2] --&gt; T2[t-1] --&gt; T3[t]\n    end\n\n    subgraph \"\u56e0\u679c\u5377\u79ef\"\n        T1 --&gt; O[\u8f93\u51fat]\n        T2 --&gt; O\n        T3 --&gt; O\n        style O fill:#f96\n    end\n</code></pre> <pre><code>class WanUpBlock(nn.Module):\n    def __init__(\n        self,\n        in_dim: int,\n        out_dim: int,\n        num_res_blocks: int,\n        dropout: float = 0.0,\n        upsample_mode: Optional[str] = None,\n        non_linearity: str = \"silu\",\n    ):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n\n        # Create layers list\n        resnets = []\n        # Add residual blocks and attention if needed\n        current_dim = in_dim\n        for _ in range(num_res_blocks + 1):\n            resnets.append(WanResidualBlock(current_dim, out_dim, dropout, non_linearity))\n            current_dim = out_dim\n\n        self.resnets = nn.ModuleList(resnets)\n\n        # Add upsampling layer if needed\n        self.upsamplers = None\n        if upsample_mode is not None:\n            self.upsamplers = nn.ModuleList([WanResample(out_dim, mode=upsample_mode)])\n\n        self.gradient_checkpointing = False\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        for resnet in self.resnets:\n            if feat_cache is not None:\n                x = resnet(x, feat_cache, feat_idx)\n            else:\n                x = resnet(x)\n\n        if self.upsamplers is not None:\n            if feat_cache is not None:\n                x = self.upsamplers[0](x, feat_cache, feat_idx)\n            else:\n                x = self.upsamplers[0](x)\n        return x\n</code></pre> <p>\u73b0\u5728\u6700\u5927\u7684\u95ee\u9898\u5c31\u662f <code>feat_cache</code></p> <p>\u8c8c\u4f3c\u6bcf\u4e00\u8f6e\u90fd\u4f1a\u6709\u4e00\u4e2acache\u8fc7\u7a0b\uff1f\u800c\u4e14\u5728\u4e00\u4e2a self.decoder \u5c31\u4f1a\u51e0\u4e4e\u6253\u6ee125\u4e2afeat_cache\u4f4d\u7f6e\u3002</p> <p>\u8fd9\u91cc\u7684\u52a8\u6001\u5f88\u591a.\u5177\u4f53\u5185\u5bb9\u5305\u62ec.</p> <p>\u4ed6\u7684shape\u4e5f\u5f88\u5947\u602a.</p> <p>paper: https://files.alicdn.com/tpsservice/5c9de1c74de03972b7aa657e5a54756b.pdf</p> <p></p> <p></p> <p>\u6574\u4f53\u7ed3\u6784\u53c2\u8003\uff1a</p> <p></p> <p>\u7b2c\u4e8c\u538b\u7f29\u5e27\u7684\u5185\u5bb9 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\n</code></pre> \u7b2c\u4e09\u538b\u7f29\u5e27 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\n</code></pre></p> <p>\u7b2c4\u5e27 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\n</code></pre></p> <p>5 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\n</code></pre></p> <p>\u76ee\u524d\u53d1\u73b0 \u7b2c\u4e00\u5e27\u548c\u7b2c\u4e8c\u5e27\u7684\u884c\u4e3a\u4e0e\u540e\u9762\u90fd\u4e0d\u592a\u4e00\u6837</p>"}]}