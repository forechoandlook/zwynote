{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"<p>wellcome to my blog</p>"},{"location":"guide/","title":"\u4f7f\u7528\u6307\u5357","text":""},{"location":"guide/#_2","title":"\u672c\u5730\u5f00\u53d1","text":""},{"location":"guide/#_3","title":"\u73af\u5883\u51c6\u5907","text":"<ol> <li>\u5b89\u88c5 Python 3.x</li> <li>\u5b89\u88c5 MkDocs \u548c Material \u4e3b\u9898\uff1a</li> </ol> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"guide/#_4","title":"\u672c\u5730\u8fd0\u884c","text":"<ol> <li>\u514b\u9686\u4ed3\u5e93\uff1a</li> </ol> <pre><code>git clone &lt;repository-url&gt;\ncd my-notes\n</code></pre> <ol> <li>\u542f\u52a8\u672c\u5730\u670d\u52a1\u5668\uff1a</li> </ol> <pre><code>mkdocs serve\n</code></pre> <ol> <li>\u5728\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee <code>http://127.0.0.1:8000</code> \u67e5\u770b\u6587\u6863</li> </ol>"},{"location":"guide/#_5","title":"\u6587\u6863\u7ed3\u6784","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md      # \u9996\u9875\n\u251c\u2500\u2500 guide.md      # \u4f7f\u7528\u6307\u5357\n\u2514\u2500\u2500 cc/           # C++ \u76f8\u5173\u6587\u6863\n    \u2514\u2500\u2500 mix.md    # Mix \u6a21\u677f\u7f16\u7a0b\n</code></pre>"},{"location":"guide/#_6","title":"\u8d21\u732e\u6307\u5357","text":""},{"location":"guide/#_7","title":"\u6dfb\u52a0\u65b0\u6587\u6863","text":"<ol> <li>\u5728 <code>docs</code> \u76ee\u5f55\u4e0b\u521b\u5efa Markdown \u6587\u4ef6</li> <li>\u5728 <code>mkdocs.yml</code> \u7684 <code>nav</code> \u90e8\u5206\u6dfb\u52a0\u6587\u6863\u94fe\u63a5</li> <li>\u63d0\u4ea4 Pull Request</li> </ol>"},{"location":"guide/#markdown","title":"Markdown \u89c4\u8303","text":"<ul> <li>\u4f7f\u7528 ATX \u98ce\u683c\u6807\u9898\uff08<code>#</code> \u53f7\uff09</li> <li>\u4ee3\u7801\u5757\u6307\u5b9a\u8bed\u8a00\u7c7b\u578b</li> <li>\u9002\u5f53\u4f7f\u7528\u8868\u683c\u3001\u5217\u8868\u7b49 Markdown \u5143\u7d20</li> <li>\u4fdd\u6301\u6587\u6863\u7ed3\u6784\u6e05\u6670</li> </ul>"},{"location":"guide/#_8","title":"\u672c\u5730\u9884\u89c8","text":"<p>\u4fee\u6539\u6587\u6863\u540e\uff0c\u672c\u5730\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u91cd\u65b0\u52a0\u8f7d\uff0c\u5b9e\u65f6\u9884\u89c8\u66f4\u6539\u3002</p>"},{"location":"guide/#_9","title":"\u90e8\u7f72","text":"<p>\u672c\u6587\u6863\u4f7f\u7528 GitHub Actions \u81ea\u52a8\u90e8\u7f72\u5230 GitHub Pages\u3002\u6bcf\u6b21\u63a8\u9001\u5230 main \u5206\u652f\u65f6\u4f1a\u81ea\u52a8\u89e6\u53d1\u90e8\u7f72\u6d41\u7a0b\u3002</p>"},{"location":"cc/cudac/","title":"4090 \u7684cuda\u7279\u6027","text":""},{"location":"cc/cudac/#api","title":"\u57fa\u672capi","text":""},{"location":"cc/cudac/#_1","title":"\u5185\u5b58\u7ba1\u7406","text":"<p>cudaMalloc\uff1a\u5728\u8bbe\u5907\uff08GPU\uff09\u4e0a\u5206\u914d\u5185\u5b58\u3002 cudaFree\uff1a\u91ca\u653e\u8bbe\u5907\u4e0a\u7684\u5185\u5b58\u3002 cudaMemcpy\uff1a\u5728\u4e3b\u673a\uff08CPU\uff09\u548c\u8bbe\u5907\u4e4b\u95f4\u590d\u5236\u5185\u5b58\u3002</p>"},{"location":"cc/cudac/#_2","title":"\u540c\u6b65","text":"<p>cudaDeviceSynchronize\uff1a\u7b49\u5f85\u8bbe\u5907\u4e0a\u7684\u6240\u6709\u5148\u524d\u53d1\u5e03\u7684\u4efb\u52a1\u5b8c\u6210\u3002 __syncthreads()\uff1a\u5728\u4e00\u4e2a\u7ebf\u7a0b\u5757\u5185\u540c\u6b65\u6240\u6709\u7ebf\u7a0b\u3002</p>"},{"location":"cc/cudac/#_3","title":"\u8bbe\u5907\u7ba1\u7406","text":"<p>cudaSetDevice\uff1a\u8bbe\u7f6e\u5f53\u524d\u4f7f\u7528\u7684GPU\u8bbe\u5907\u3002 cudaGetDeviceProperties\uff1a\u83b7\u53d6\u8bbe\u5907\u7684\u5c5e\u6027\u3002</p>"},{"location":"cc/cudac/#_4","title":"\u9519\u8bef\u5904\u7406","text":"<p>cudaGetLastError\uff1a\u8fd4\u56de\u4e0a\u4e00\u4e2aCUDA\u8c03\u7528\u7684\u9519\u8bef\u72b6\u6001\u3002 cudaGetErrorString\uff1a\u8fd4\u56de\u9519\u8bef\u4ee3\u7801\u5bf9\u5e94\u7684\u5b57\u7b26\u4e32\u63cf\u8ff0\u3002</p>"},{"location":"cc/cudac/#stream","title":"\u6d41\uff08Stream\uff09\u7ba1\u7406","text":"<p>cudaStreamCreate\uff1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u6d41\u3002 cudaStreamDestroy\uff1a\u9500\u6bc1\u4e00\u4e2a\u6d41\u3002 cudaStreamSynchronize\uff1a\u7b49\u5f85\u4e00\u4e2a\u6d41\u4e2d\u7684\u6240\u6709\u64cd\u4f5c\u5b8c\u6210\u3002</p>"},{"location":"cc/cudac/#event","title":"\u4e8b\u4ef6\uff08Event\uff09\u7ba1\u7406","text":"<p>cudaEventCreate\uff1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u4e8b\u4ef6\u3002 cudaEventRecord\uff1a\u5728\u4e00\u4e2a\u6d41\u4e2d\u8bb0\u5f55\u4e00\u4e2a\u4e8b\u4ef6\u3002 cudaEventSynchronize\uff1a\u7b49\u5f85\u4e00\u4e2a\u4e8b\u4ef6\u5b8c\u6210\u3002</p>"},{"location":"cc/cudac/#_5","title":"\u539f\u5b50\u64cd\u4f5c","text":"<p>atomicAdd\uff1a\u539f\u5b50\u5730\u589e\u52a0\u4e00\u4e2a\u503c\u3002 atomicCAS\uff1a\u539f\u5b50\u5730\u6bd4\u8f83\u5e76\u4ea4\u6362\u4e00\u4e2a\u503c\u3002</p>"},{"location":"cc/cudac/#_6","title":"\u4ece\u6307\u4ee4\u89d2\u5ea6\u7406\u89e3","text":"<p>ptx \u72ec\u7acb\u4e8e\u7279\u5b9agpu\u67b6\u6784</p> <ol> <li>nvcc \u7f16\u8bd1 .cu \u6587\u4ef6 ===&gt; .ptx \u4ee3\u7801</li> <li>.ptx -&gt; gpu\u67b6\u6784\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801 \u9a71\u52a8\u7a0b\u5e8f\u5728\u8fd0\u884c\u65f6\u5b8c\u6210\uff0c\u6216\u8005\u7f16\u8bd1\u65f6\u901a\u8fc7\u6307\u5b9a\u76ee\u6807\u67b6\u6784 <code>-arch=sm_75</code>\u5b8c\u6210</li> </ol>"},{"location":"cc/cudac/#ptx","title":"ptx\u6307\u4ee4","text":"<p>PTX\u6307\u4ee4\u96c6\u5305\u62ec\u7b97\u672f\u6307\u4ee4\u3001\u5185\u5b58\u8bbf\u95ee\u6307\u4ee4\u3001\u63a7\u5236\u6d41\u6307\u4ee4\u7b49\u3002\u4f8b\u5982\uff1a add.f32\uff1a\u6d6e\u70b9\u6570\u52a0\u6cd5\u3002 ld.global\uff1a\u4ece\u5168\u5c40\u5185\u5b58\u52a0\u8f7d\u6570\u636e\u3002 st.shared\uff1a\u5b58\u50a8\u6570\u636e\u5230\u5171\u4eab\u5185\u5b58\u3002 bar.sync\uff1a\u7ebf\u7a0b\u5757\u5185\u7684\u540c\u6b65\u5c4f\u969c\u3002</p> <p>\u4f18\u5316\u548c\u8c03\u5ea6\uff1a</p> <p>\u5728PTX\u5230\u4e8c\u8fdb\u5236\u4ee3\u7801\u7684\u8f6c\u6362\u8fc7\u7a0b\u4e2d\uff0c\u7f16\u8bd1\u5668\u4f1a\u8fdb\u884c\u5404\u79cd\u4f18\u5316\uff0c\u5982\u6307\u4ee4\u8c03\u5ea6\u3001\u5bc4\u5b58\u5668\u5206\u914d\u7b49\uff0c\u4ee5\u63d0\u9ad8\u4ee3\u7801\u7684\u6267\u884c\u6548\u7387\u3002 \u8fd9\u4e9b\u4f18\u5316\u65e8\u5728\u6700\u5927\u5316GPU\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\uff0c\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u548c\u7ebf\u7a0b\u7b49\u5f85\u65f6\u95f4\u3002</p> <p>\u8fd0\u884c\u65f6\u52a0\u8f7d\uff1a</p> <p>\u5728\u7a0b\u5e8f\u8fd0\u884c\u65f6\uff0cCUDA\u9a71\u52a8\u7a0b\u5e8f\u4f1a\u52a0\u8f7d\u4e8c\u8fdb\u5236\u4ee3\u7801\u5230GPU\uff0c\u5e76\u7ba1\u7406\u7ebf\u7a0b\u7684\u8c03\u5ea6\u548c\u6267\u884c\u3002 PTX\u4ee3\u7801\u4e5f\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u7531\u9a71\u52a8\u7a0b\u5e8f\u8fdb\u884c\u5373\u65f6\u7f16\u8bd1\uff08JIT\uff09\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684GPU\u67b6\u6784\u3002</p> <p>\u5177\u4f53\u6307\u4ee4\uff1a \u7b97\u672f\u6307\u4ee4\uff1a</p> <p>add\uff1a\u6574\u6570\u52a0\u6cd5\u3002 sub\uff1a\u6574\u6570\u51cf\u6cd5\u3002 mul\uff1a\u6574\u6570\u4e58\u6cd5\u3002 mad\uff1a\u4e58\u52a0\uff08multiply-add\uff09\u3002 fadd\uff1a\u6d6e\u70b9\u6570\u52a0\u6cd5\u3002 fmul\uff1a\u6d6e\u70b9\u6570\u4e58\u6cd5\u3002 \u5185\u5b58\u8bbf\u95ee\u6307\u4ee4\uff1a</p> <p>ld\uff1a\u4ece\u5185\u5b58\u52a0\u8f7d\u6570\u636e\u3002 st\uff1a\u5c06\u6570\u636e\u5b58\u50a8\u5230\u5185\u5b58\u3002 ld.global\uff1a\u4ece\u5168\u5c40\u5185\u5b58\u52a0\u8f7d\u6570\u636e\u3002 st.shared\uff1a\u5c06\u6570\u636e\u5b58\u50a8\u5230\u5171\u4eab\u5185\u5b58\u3002 \u63a7\u5236\u6d41\u6307\u4ee4\uff1a</p> <p>bra\uff1a\u65e0\u6761\u4ef6\u8df3\u8f6c\u3002 setp\uff1a\u8bbe\u7f6e\u8c13\u8bcd\u5bc4\u5b58\u5668\u3002 @p\uff1a\u57fa\u4e8e\u8c13\u8bcd\u7684\u6761\u4ef6\u6267\u884c\u3002 \u540c\u6b65\u6307\u4ee4\uff1a</p> <p>bar.sync\uff1a\u7ebf\u7a0b\u5757\u5185\u7684\u540c\u6b65\u5c4f\u969c\u3002 membar\uff1a\u5185\u5b58\u5c4f\u969c\uff0c\u786e\u4fdd\u5185\u5b58\u64cd\u4f5c\u7684\u987a\u5e8f\u3002 \u539f\u5b50\u64cd\u4f5c\u6307\u4ee4\uff1a</p> <p>atom.add\uff1a\u539f\u5b50\u52a0\u6cd5\u3002 atom.cas\uff1a\u539f\u5b50\u6bd4\u8f83\u5e76\u4ea4\u6362\u3002 \u8f6c\u6362\u6307\u4ee4\uff1a</p> <p>cvt\uff1a\u7c7b\u578b\u8f6c\u6362\uff08\u5982\u6574\u6570\u5230\u6d6e\u70b9\u6570\uff09\u3002 float2int\uff1a\u6d6e\u70b9\u6570\u5230\u6574\u6570\u7684\u8f6c\u6362\u3002 \u7279\u6b8a\u6307\u4ee4\uff1a</p> <p>tex\uff1a\u7eb9\u7406\u5185\u5b58\u8bbf\u95ee\u3002 s2r\uff1a\u7279\u6b8a\u5bc4\u5b58\u5668\u8bbf\u95ee\u3002 \u903b\u8f91\u6307\u4ee4\uff1a</p> <p>and\uff1a\u6309\u4f4d\u4e0e\u3002 or\uff1a\u6309\u4f4d\u6216\u3002 not\uff1a\u6309\u4f4d\u975e\u3002</p>"},{"location":"cc/cudac/#_7","title":"\u7ee7\u7eed\u5b66\u4e60","text":"<p>\u4e00\u4e9b\u51fd\u6570</p> <pre><code>#include &lt;ATen/Tensor.h&gt;\n#include &lt;ATen/Functions.h&gt;\n#include &lt;torch/csrc/utils/pybind.h&gt;\n\nvoid square_cuda_forward(void* input, void* output, int size);\n\nat::Tensor square_forward(const at::Tensor&amp; input) {\n    auto output = at::zeros_like(input);\n\n    square_cuda_forward(input.data_ptr(), output.data_ptr(), input.numel());\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"square_forward\", &amp;square_forward, \"Square forward (CUDA)\");\n}\n</code></pre> <pre><code>os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.9\"\nstart = time.time()\n\nsquare_cuda = load(\n    name=\"square_cuda\",\n    sources=[f\"{dir_path}/square_kernel.cu\"],\n    verbose=True,\n    build_directory=build_dir\n)\n\nend = time.time()\nprint(f\"Time taken: {end - start} seconds\")\n</code></pre>"},{"location":"cc/cudac/#tilelang","title":"tilelang \u5b66\u4e60","text":"<p>from : https://leiblog.wang/Debug-Tools-for-TileLang/</p> <ul> <li>ctypes \u8c03\u7528\u7684\u7406\u8bba\u5f00\u9500\u5e94\u8be5\u5728 500ns ~ 5\u00b5s \u4e4b\u95f4\uff0c\u57fa\u672c\u53ef\u5ffd\u7565\u3002</li> </ul> <p></p>"},{"location":"cc/cudac/#best-practice","title":"best practice","text":"<p>https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/</p>"},{"location":"cc/mix/","title":"Mix","text":""},{"location":"cc/mix/#mix","title":"mix","text":"<p>from : https://zhuanlan.zhihu.com/p/460825741</p> <p>\u7528\u9014\uff1a\u5c06\u82e5\u5e72\u529f\u80fd\u72ec\u7acb\u7684\u7c7b\u901a\u8fc7\u7ee7\u627f\u7684\u65b9\u5f0f\u5b9e\u73b0\u6a21\u5757\u590d\u7528\u7684C++\u6a21\u677f\u7f16\u7a0b\u6280\u5de7</p> <pre><code>template&lt;typename... Mixins&gt;\nclass MixinClass : public Mixins... {\n  public:\n    MixinClass() :  Mixins...() {}\n  // ...\n};\n</code></pre> <p><code>\u5c06\u6a21\u677f\u53c2\u6570\u4f5c\u4e3a\u6d3e\u751f\u7c7b\u7684\u57fa\u7c7b</code></p> <pre><code>template &lt;typename... Mixins&gt;\nclass Point : public Mixins... {\n public:\n  double x, y;\n  Point() : Mixins()..., x(0.0), y(0.0) {}\n  Point(double x, double y) : Mixins()..., x(x), y(y) {}\n};\n\nclass Label {\n public:\n  std::string label;\n  Label() : label(\"\") {}\n};\n\nclass Color {\n public:\n  unsigned char red = 0, green = 0, blue = 0;\n};\n\nusing MyPoint = Point&lt;Label, Color&gt;;\n</code></pre> <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nstruct Number\n{\n    typedef int value_type;\n    int n;\n    void set(int v) { n = v; }\n    int get() const { return n; }\n};\n\ntemplate &lt;typename BASE, typename T = typename BASE::value_type&gt;\nstruct Undoable\n{\n    typedef T value_type;\n    BASE base;\n    T before;\n    void set(T v) { before = base.get(); base.set(v); }\n    void undo() { base.set(before); }\n    T get() const { return base.get(); }\n};\n\ntemplate &lt;typename BASE, typename T = typename BASE::value_type&gt;\nstruct Redoable\n{\n    typedef T value_type;\n    BASE base;\n    T after;\n    void set(T v) { after = v; base.set(v); }\n    void redo() { base.set(after); }\n    T get() const { return base.get(); }\n};\n\ntypedef Redoable&lt; Undoable&lt;Number&gt; &gt; ReUndoableNumber;\n\nint main()\n{\n    ReUndoableNumber mynum;\n    mynum.set(42); mynum.set(84);\n    cout &lt;&lt; mynum.get() &lt;&lt; '\\n';  // 84\n    mynum.undo();\n    cout &lt;&lt; mynum.get() &lt;&lt; '\\n';  // 42\n    mynum.redo();\n    cout &lt;&lt; mynum.get() &lt;&lt; '\\n';  // back to 84\n}\n</code></pre> <p>\u8fd8\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u5b9e\u73b0\u53ef\u8ddf\u8e2a\u7684\u5f02\u5e38,\u4f46\u662f\u6682\u65f6\u4e0dcare\u4e86</p>"},{"location":"cc/smart_point/","title":"Smart point","text":""},{"location":"cc/smart_point/#_1","title":"\u667a\u80fd\u6307\u9488","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;cstring&gt;\n#include &lt;atomic&gt;\n\n// ------------------- UniquePtr \u5b9e\u73b0 -------------------\ntemplate &lt;typename T&gt;\nclass UniquePtr {\nprivate:\n    T* ptr;\npublic:\n    explicit UniquePtr(T* p = nullptr) : ptr(p) {}\n    ~UniquePtr() { delete ptr; }\n\n    UniquePtr(const UniquePtr&amp;) = delete;\n    UniquePtr&amp; operator=(const UniquePtr&amp;) = delete;\n\n    UniquePtr(UniquePtr&amp;&amp; other) noexcept : ptr(other.ptr) {\n        other.ptr = nullptr;\n    }\n    UniquePtr&amp; operator=(UniquePtr&amp;&amp; other) noexcept {\n        if (this != &amp;other) {\n            delete ptr;\n            ptr = other.ptr;\n            other.ptr = nullptr;\n        }\n        return *this;\n    }\n\n    T* operator-&gt;() const { return ptr; }\n    T&amp; operator*() const { return *ptr; }\n\n    T* get() const { return ptr; }\n};\n\n// ------------------- SharedPtr \u5b9e\u73b0 -------------------\ntemplate &lt;typename T&gt;\nclass SharedPtr {\nprivate:\n    T* ptr;\n    std::atomic&lt;int&gt;* count;\npublic:\n    explicit SharedPtr(T* p = nullptr) : ptr(p), count(new std::atomic&lt;int&gt;(p ? 1 : 0)) {}\n\n    ~SharedPtr() {\n        if (--(*count) == 0) {\n            delete ptr;\n            delete count;\n        }\n    }\n\n    SharedPtr(const SharedPtr&amp; other) : ptr(other.ptr), count(other.count) {\n        ++(*count);\n    }\n\n    SharedPtr&amp; operator=(const SharedPtr&amp; other) {\n        if (this != &amp;other) {\n            if (--(*count) == 0) {\n                delete ptr;\n                delete count;\n            }\n            ptr = other.ptr;\n            count = other.count;\n            ++(*count);\n        }\n        return *this;\n    }\n\n    T* operator-&gt;() const { return ptr; }\n    T&amp; operator*() const { return *ptr; }\n\n    T* get() const { return ptr; }\n};\n\n// ------------------- MyString \u5b9e\u73b0 -------------------\nclass MyString {\nprivate:\n    char* data;\n    size_t length;\npublic:\n    MyString(const char* str = \"\") {\n        length = std::strlen(str);\n        data = new char[length + 1];\n        std::strcpy(data, str);\n    }\n\n    MyString(const MyString&amp; other) {\n        length = other.length;\n        data = new char[length + 1];\n        std::strcpy(data, other.data);\n    }\n\n    MyString&amp; operator=(const MyString&amp; other) {\n        if (this != &amp;other) {\n            delete[] data;\n            length = other.length;\n            data = new char[length + 1];\n            std::strcpy(data, other.data);\n        }\n        return *this;\n    }\n\n    MyString(MyString&amp;&amp; other) noexcept : data(other.data), length(other.length) {\n        other.data = nullptr;\n        other.length = 0;\n    }\n\n    MyString&amp; operator=(MyString&amp;&amp; other) noexcept {\n        if (this != &amp;other) {\n            delete[] data;\n            data = other.data;\n            length = other.length;\n            other.data = nullptr;\n            other.length = 0;\n        }\n        return *this;\n    }\n\n    ~MyString() { delete[] data; }\n\n    const char* c_str() const { return data; }\n};\n\n// ------------------- \u6d4b\u8bd5\u4ee3\u7801 -------------------\nint main() {\n    UniquePtr&lt;int&gt; uPtr(new int(10));\n    std::cout &lt;&lt; \"UniquePtr: \" &lt;&lt; *uPtr &lt;&lt; std::endl;\n\n    SharedPtr&lt;int&gt; sPtr1(new int(20));\n    SharedPtr&lt;int&gt; sPtr2 = sPtr1;\n    std::cout &lt;&lt; \"SharedPtr: \" &lt;&lt; *sPtr1 &lt;&lt; std::endl;\n\n    MyString str1(\"Hello, World!\");\n    MyString str2 = str1;\n    std::cout &lt;&lt; \"MyString: \" &lt;&lt; str2.c_str() &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre> <pre><code>template &lt;typename T&gt;\nconstexpr typename std::remove_reference&lt;T&gt;::type&amp;&amp; move(T&amp;&amp; arg) noexcept {\n    return static_cast&lt;typename std::remove_reference&lt;T&gt;::type&amp;&amp;&gt;(arg);\n}\n</code></pre> <p>std::move \u65e0\u6761\u4ef6 \u8f6c\u6362\u4e3a\u53f3\u503c\u3002 std::forward \u6709\u6761\u4ef6 \u8f6c\u6362\u4e3a\u53f3\u503c\uff0c\u4ec5\u7528\u4e8e \u5b8c\u7f8e\u8f6c\u53d1\uff08\u914d\u5408\u6a21\u677f\uff09\u3002</p> <pre><code>template &lt;typename T&gt;\nvoid foo(T&amp;&amp; arg) {\n    T new_value = std::forward&lt;T&gt;(arg); // \u4fdd\u6301\u5de6\u503c/\u53f3\u503c\u5c5e\u6027\n}\n</code></pre>"},{"location":"cc/smart_point/#_2","title":"\u4e07\u80fd\u5f15\u7528\u548c\u53f3\u503c\u5f15\u7528","text":"<p>\u7ed1\u5b9a\u53f3\u503c\uff08\u6ca1\u6709\u540d\u5b57\u7684\u4e34\u65f6\u5bf9\u8c61\uff09 \u7684\u5f15\u7528\uff0c\u5141\u8bb8 \u79fb\u52a8\u8bed\u4e49\u3002 1. \u4e13\u95e8\u7528\u4e8e\u53f3\u503c\uff08int&amp;&amp; \u53ea\u80fd\u7ed1\u5b9a 10\uff0c\u4e0d\u80fd\u7ed1\u5b9a int a;\uff09\u3002 2. \u901a\u5e38\u7528\u4e8e\u79fb\u52a8\u6784\u9020\u3001\u79fb\u52a8\u8d4b\u503c\uff0c\u907f\u514d\u62f7\u8d1d\uff0c\u63d0\u9ad8\u6027\u80fd\u3002 <pre><code>void foo(int&amp;&amp; x) { // \u53f3\u503c\u5f15\u7528\n    std::cout &lt;&lt; x &lt;&lt; std::endl;\n}\n\nint main() {\n    int a = 10;\n    foo(10);   // \u2705 \u53f3\u503c OK\n    foo(a);    // \u274c \u5de6\u503c\u4e0d\u80fd\u4f20\u7ed9 int&amp;&amp;\n}\n</code></pre> \u5f53 T&amp;&amp; \u51fa\u73b0\u5728\u51fd\u6570\u6a21\u677f\u4e2d\uff0c\u5e76\u4e14 T \u662f\u6a21\u677f\u53c2\u6570\uff0c\u5b83\u53d8\u6210 \u4e07\u80fd\u5f15\u7528\uff0c\u53ef\u4ee5\u63a5\u53d7\u5de6\u503c\u548c\u53f3\u503c\u3002 1. T&amp;&amp; \u5728\u6a21\u677f\u53c2\u6570\u4e2d\u65f6\u662f\u4e07\u80fd\u5f15\u7528\uff0c\u53ef\u4ee5\u63a5\u53d7\u5de6\u503c\u548c\u53f3\u503c\u3002 2. \u9700\u8981 std::forward(arg) \u6765\u4fdd\u6301\u539f\u6709\u7684\u5de6\u503c/\u53f3\u503c\u7279\u6027\u3002 3. \u7528\u4e8e\u6cdb\u578b\u7f16\u7a0b\uff0c\u4f7f\u51fd\u6570\u53ef\u4ee5\u63a5\u53d7\u5404\u79cd\u7c7b\u578b\u7684\u53c2\u6570\u3002 <pre><code>template &lt;typename T&gt;\nvoid wrapper(T&amp;&amp; arg) { // T&amp;&amp; \u662f\u4e07\u80fd\u5f15\u7528\n    foo(std::forward&lt;T&gt;(arg)); // \u4fdd\u6301\u5de6\u503c\u6216\u53f3\u503c\u7684\u7279\u6027\n}\n\nint main() {\n    int a = 10;\n    wrapper(a);   // \u4f20\u5165\u5de6\u503c\uff0cT = int&amp;\n    wrapper(20);  // \u4f20\u5165\u53f3\u503c\uff0cT = int\n}\n</code></pre> <p><pre><code>template &lt;typename T, typename... Args&gt;\nvoid emplace_back(Args&amp;&amp;... args) {\n    data.push_back(T(std::forward&lt;Args&gt;(args)...));\n}\n</code></pre> \u76f4\u63a5\u5728\u5bb9\u5668\u4e2d\u6784\u9020\u5bf9\u8c61\uff0c\u907f\u514d\u4e86\u62f7\u8d1d\u548c\u79fb\u52a8\u64cd\u4f5c\u3002</p> <p><code>vec.emplace_back(1, 2.5, \"Hello\");  // \u76f4\u63a5\u5728\u5bb9\u5668\u4e2d\u6784\u9020\uff0c\u907f\u514d\u4e34\u65f6\u5bf9\u8c61</code></p> <p>\u5728\u9ad8\u6027\u80fd\u5e94\u7528\u4e2d\uff0c\u4f7f\u7528 emplace_back \u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u62f7\u8d1d\uff0c\u63d0\u9ad8\u4ee3\u7801\u7684\u6548\u7387</p> <pre><code>#include &lt;iostream&gt;\n\ntemplate &lt;typename T&gt;\nclass SharedPtr {\nprivate:\n    T* ptr;\n    int* ref_count;\n\npublic:\n    explicit SharedPtr(T* p = nullptr) : ptr(p), ref_count(new int(1)) {}\n\n    ~SharedPtr() {\n        if (--(*ref_count) == 0) {\n            delete ptr;\n            delete ref_count;\n        }\n    }\n\n    SharedPtr(const SharedPtr&amp; other) noexcept : ptr(other.ptr), ref_count(other.ref_count) {\n        ++(*ref_count);\n    }\n\n    SharedPtr&amp; operator=(const SharedPtr&amp; other) noexcept {\n        if (this != &amp;other) {\n            if (--(*ref_count) == 0) {\n                delete ptr;\n                delete ref_count;\n            }\n            ptr = other.ptr;\n            ref_count = other.ref_count;\n            ++(*ref_count);\n        }\n        return *this;\n    }\n\n    T* get() const { return ptr; }\n    T* operator-&gt;() const { return ptr; }\n    T&amp; operator*() const { return *ptr; }\n\n    int use_count() const { return *ref_count; }\n};\n\nstruct Test {\n    void show() { std::cout &lt;&lt; \"SharedPtr works!\\n\"; }\n};\n\nint main() {\n    SharedPtr&lt;Test&gt; p1(new Test());\n    SharedPtr&lt;Test&gt; p2 = p1;\n    p2-&gt;show();\n    std::cout &lt;&lt; \"Reference count: \" &lt;&lt; p1.use_count() &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\ntemplate &lt;typename T&gt;\nclass SharedPtr; // \u524d\u5411\u58f0\u660e\n\ntemplate &lt;typename T&gt;\nclass WeakPtr {\nprivate:\n    T* ptr;\n    int* ref_count;\n    int* weak_count; // \u989d\u5916\u7684\u5f15\u7528\u8ba1\u6570\uff0c\u7528\u6765\u7ba1\u7406 weak_ptr \u7684\u6570\u91cf\n\npublic:\n    WeakPtr() : ptr(nullptr), ref_count(nullptr), weak_count(nullptr) {}\n\n    WeakPtr(const SharedPtr&lt;T&gt;&amp; shared) : ptr(shared.ptr), ref_count(shared.ref_count), weak_count(shared.weak_count) {\n        if (weak_count) {\n            ++(*weak_count); // \u589e\u52a0 weak_ptr \u6570\u91cf\n        }\n    }\n\n    ~WeakPtr() {\n        if (weak_count &amp;&amp; --(*weak_count) == 0) {\n            delete weak_count;\n        }\n    }\n\n    // lock() \u65b9\u6cd5\uff1a\u8fd4\u56de\u4e00\u4e2a shared_ptr\uff0c\u5982\u679c\u8d44\u6e90\u5df2\u7ecf\u88ab\u9500\u6bc1\uff0c\u8fd4\u56de\u7a7a\u7684 shared_ptr\n    SharedPtr&lt;T&gt; lock() const;\n\n    // \u83b7\u53d6\u5bf9\u8c61\u662f\u5426\u6709\u6548\n    bool expired() const {\n        return *ref_count == 0;\n    }\n};\n\ntemplate &lt;typename T&gt;\nclass SharedPtr {\nprivate:\n    T* ptr;\n    int* ref_count; // \u5f15\u7528\u8ba1\u6570\n    int* weak_count; // weak_ptr \u5f15\u7528\u8ba1\u6570\n\npublic:\n    explicit SharedPtr(T* p = nullptr) : ptr(p), ref_count(new int(1)), weak_count(new int(0)) {}\n\n    ~SharedPtr() {\n        if (--(*ref_count) == 0) {\n            delete ptr;\n            delete ref_count;\n            if (*weak_count == 0) {\n                delete weak_count;\n            }\n        }\n    }\n\n    SharedPtr(const SharedPtr&amp; other) noexcept : ptr(other.ptr), ref_count(other.ref_count), weak_count(other.weak_count) {\n        ++(*ref_count);\n    }\n\n    SharedPtr&amp; operator=(const SharedPtr&amp; other) noexcept {\n        if (this != &amp;other) {\n            if (--(*ref_count) == 0) {\n                delete ptr;\n                delete ref_count;\n                if (*weak_count == 0) {\n                    delete weak_count;\n                }\n            }\n            ptr = other.ptr;\n            ref_count = other.ref_count;\n            weak_count = other.weak_count;\n            ++(*ref_count);\n        }\n        return *this;\n    }\n\n    SharedPtr(SharedPtr&amp;&amp; other) noexcept : ptr(other.ptr), ref_count(other.ref_count), weak_count(other.weak_count) {\n        other.ptr = nullptr;\n        other.ref_count = nullptr;\n        other.weak_count = nullptr;\n    }\n\n    SharedPtr&amp; operator=(SharedPtr&amp;&amp; other) noexcept {\n        if (this != &amp;other) {\n            if (--(*ref_count) == 0) {\n                delete ptr;\n                delete ref_count;\n                if (*weak_count == 0) {\n                    delete weak_count;\n                }\n            }\n            ptr = other.ptr;\n            ref_count = other.ref_count;\n            weak_count = other.weak_count;\n            other.ptr = nullptr;\n            other.ref_count = nullptr;\n            other.weak_count = nullptr;\n        }\n        return *this;\n    }\n\n    friend class WeakPtr&lt;T&gt;;\n\n    // \u8fd4\u56de\u539f\u59cb\u6307\u9488\n    T* get() const { return ptr; }\n    int use_count() const { return *ref_count; }\n};\n\n// weak_ptr::lock \u5b9e\u73b0\ntemplate &lt;typename T&gt;\nSharedPtr&lt;T&gt; WeakPtr&lt;T&gt;::lock() const {\n    if (*ref_count &gt; 0) {\n        return SharedPtr&lt;T&gt;(*this); // \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 shared_ptr\n    } else {\n        return SharedPtr&lt;T&gt;(); // \u8fd4\u56de\u4e00\u4e2a\u7a7a\u7684 shared_ptr\n    }\n}\n\nstruct Test {\n    void show() { std::cout &lt;&lt; \"Test class works!\\n\"; }\n};\n\nint main() {\n    SharedPtr&lt;Test&gt; sp1(new Test());\n    {\n        WeakPtr&lt;Test&gt; wp1(sp1); // wp1 \u89c2\u5bdf sp1\n        if (!wp1.expired()) {\n            SharedPtr&lt;Test&gt; sp2 = wp1.lock(); // \u4ece weak_ptr \u83b7\u53d6 shared_ptr\n            sp2-&gt;show(); // \u8f93\u51fa\uff1aTest class works!\n        }\n    } // wp1 \u79bb\u5f00\u4f5c\u7528\u57df\uff0c\u5f31\u5f15\u7528\u4e0d\u518d\u5f15\u7528\u8d44\u6e90\n\n    if (sp1.use_count() == 0) {\n        std::cout &lt;&lt; \"sp1 has no references left.\" &lt;&lt; std::endl;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"cc/type_erasure/","title":"Type erasure","text":"<pre><code>std::any a;             // a is empty\na = 4.3;                // a has value 4.3 of type double\na = 42;                 // a has value 42 of type int\na = std::string{\"hi\"};  // a has value \"hi\" of type std::string\nif (a.type() == typeid(std::string)) {\n  std::string s = std::any_cast&lt;std::string&gt;(a);\n  UseString(s);\n} else if (a.type() == typeid(int)) {\n  UseInt(std::any_cast&lt;int&gt;(a));\n}\n</code></pre> <p>std::any\u53ef\u4ee5\u7528\u6765\u8868\u793a\u4efb\u4f55\u53ef\u62f7\u8d1d\u6784\u9020\u7684\u5355\u503c\u7c7b\u578b,\u5bf9\u7c7b\u578b\u7684\u6570\u636e\u8fdb\u884c\u4e86\u62bd\u8c61\u3002\u9664\u4e86\u5bf9\u7c7b\u578b\u7684\u6570\u636e\u8fdb\u884c\u62bd\u8c61\u5916\uff0c\u4e5f\u53ef\u4ee5\u5bf9\u7c7b\u578b\u7684\u884c\u4e3a\u8fdb\u884c\u62bd\u8c61\uff0c\u4f8b\u5982std::function\u53ef\u4ee5\u7528\u6765\u8868\u793a\u6240\u6709\u7684\u53ef\u88ab\u8c03\u7528\u7684\u5bf9\u8c61\uff1a\u666e\u901a\u51fd\u6570\u3001\u6210\u5458\u51fd\u6570\u3001\u51fd\u6570\u5bf9\u8c61\u3001lambda\u8868\u8fbe\u5f0f\u3002</p> <p>\u5177\u4f53\u5b9e\u73b0\u662f:</p> <pre><code>class Any {\n  // Holds either pointer to a heap object or the contained object itself.\n  union Storage {\n    constexpr Storage() : ptr{nullptr} {}\n\n    // Prevent trivial copies of this type, buffer might hold a non-POD.\n    Storage(const Storage&amp;) = delete;\n    Storage&amp; operator=(const Storage&amp;) = delete;\n\n    void* ptr;\n    std::aligned_storage_t&lt;sizeof(ptr), alignof(void*)&gt; buffer;\n  };\n  Storage storage_;\n};\n</code></pre> <p>\u8fd9\u4e2a\u65b9\u6cd5\u7684\u4f18\u70b9\u662f\uff1a 1. \u5c0f\u5bf9\u8c61\u4f18\u5316\uff1a\u76f4\u63a5\u5b58\u50a8\u5728 buffer \u4e2d\uff08\u5f53\u5bf9\u8c61\u5927\u5c0f\u4e0d\u8d85\u8fc7\u6307\u9488\u5927\u5c0f\u65f6\uff09 2. \u5927\u5bf9\u8c61\u5b58\u50a8\uff1a\u4f7f\u7528 ptr \u6307\u5411\u5806\u4e0a\u5206\u914d\u7684\u5185\u5b58</p> <pre><code>class Any {\n  union Storage { /*...*/ };\n\n  // \u7528\u4e8e\u5b58\u50a8\u7c7b\u578b\u4fe1\u606f\n  const std::type_info* type_info_;\n\n  // \u7c7b\u578b\u64e6\u9664\u7684\u5173\u952e\uff1a\u51fd\u6570\u6307\u9488\u8868\n  struct VTable {\n      void (*destroy)(Storage&amp;);\n      void (*copy)(Storage&amp;, const Storage&amp;);\n      void (*move)(Storage&amp;, Storage&amp;);\n      const std::type_info&amp; (*type)();\n  };\n  const VTable* vtable_;\n\n  Storage storage_;\n};\n</code></pre> <ol> <li>\u5de5\u4f5c\u539f\u7406</li> <li>\u5f53\u5b58\u50a8\u4e00\u4e2a\u65b0\u503c\u65f6\uff1a</li> <li>\u6839\u636e\u503c\u7684\u5927\u5c0f\u9009\u62e9\u5b58\u50a8\u7b56\u7565</li> <li>\u521b\u5efa\u5bf9\u5e94\u7c7b\u578b\u7684 VTable</li> <li>\u4fdd\u5b58\u7c7b\u578b\u4fe1\u606f</li> <li>\u5728 storage_ \u4e2d\u5b58\u50a8\u6570\u636e</li> </ol> <p>\u7c7b\u578b\u64e6\u9664\u7684\u6838\u5fc3\u5728\u4e8e\uff1a</p> <ol> <li>\u7075\u6d3b\u7684\u5b58\u50a8\u7b56\u7565\uff08union\uff09</li> <li>\u865a\u51fd\u6570\u8868\uff08VTable\uff09\u8bb0\u5f55\u7c7b\u578b\u76f8\u5173\u7684\u64cd\u4f5c</li> <li>\u8fd0\u884c\u65f6\u7c7b\u578b\u4fe1\u606f\uff08type_info\uff09</li> </ol>"},{"location":"cc/type_erasure/#_1","title":"\u65b0\u7684\u7406\u89e3","text":"<pre><code>template &lt;std::size_t Len, std::size_t Align = alignof(std::max_align_t)&gt;\nstruct aligned_storage {\n    using type = typename std::aligned_storage&lt;Len, Align&gt;::type;\n};\n</code></pre> <pre><code>#include &lt;iostream&gt;\n#include &lt;type_traits&gt;\n\nclass Any {\nprivate:\n    union Storage {\n        void* ptr;\n        std::aligned_storage_t&lt;sizeof(void*), alignof(void*)&gt; buffer;\n\n        constexpr Storage() : ptr(nullptr) {}\n        ~Storage() {}\n    };\n\n    Storage storage_;\n    size_t size_;\n    void (*destroyer_)(void*);  // \u7528\u4e8e\u9500\u6bc1\u5b58\u50a8\u7684\u5bf9\u8c61\n\npublic:\n    // \u6784\u9020\u51fd\u6570\n    template &lt;typename T&gt;\n    Any(T&amp;&amp; value) {\n        size_ = sizeof(T);\n        destroyer_ = [](void* ptr) { delete static_cast&lt;T*&gt;(ptr); };\n\n        // \u4f7f\u7528 aligned_storage_t \u5b58\u50a8\u5bf9\u8c61\n        new (&amp;storage_.buffer) T(std::forward&lt;T&gt;(value));\n    }\n\n    // \u79fb\u52a8\u6784\u9020\u51fd\u6570\n    Any(Any&amp;&amp; other) noexcept {\n        size_ = other.size_;\n        destroyer_ = other.destroyer_;\n        new (&amp;storage_.buffer) void* (std::move(other.storage_.ptr));\n        other.size_ = 0;\n        other.destroyer_ = nullptr;\n    }\n\n    // \u6790\u6784\u51fd\u6570\n    ~Any() {\n        if (destroyer_) {\n            destroyer_(&amp;storage_.buffer);\n        }\n    }\n\n    // \u83b7\u53d6\u5b58\u50a8\u5bf9\u8c61\n    template &lt;typename T&gt;\n    T&amp; get() {\n        return *reinterpret_cast&lt;T*&gt;(&amp;storage_.buffer);\n    }\n};\n\nint main() {\n    Any a1 = 42;            // \u5b58\u50a8\u4e00\u4e2a int\n    Any a2 = std::string(\"Hello World\");  // \u5b58\u50a8\u4e00\u4e2a string\n\n    std::cout &lt;&lt; \"Stored int: \" &lt;&lt; a1.get&lt;int&gt;() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Stored string: \" &lt;&lt; a2.get&lt;std::string&gt;() &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre> <p>\u6700\u540e\u7528\u7684\u65f6\u5019 \u518d\u53bb\u5b8c\u5168\u7406\u89e3\u5427\u3002</p>"},{"location":"daily/0327/","title":"0327","text":""},{"location":"daily/0327/#todo","title":"todo","text":"<ol> <li>\u5b66\u4e60 code/skiplist</li> </ol>"},{"location":"daily/0327/#gpubenchmark","title":"gpubenchmark","text":"<p>https://github.com/RRZE-HPC/gpu-benches/tree/master</p>"},{"location":"daily/0327/#llvm","title":"llvm","text":"<p>https://llvm-study-notes.readthedocs.io/en/latest/</p>"},{"location":"daily/0327/#cuda-content","title":"cuda content","text":"<p><code>cuda-gdb</code> <code>cuda-memcheck</code></p> <p><code>ncu --target-processes all -o profile_output python3 elu.py</code> \u5206\u6790</p> <p><code>profile_output.ncu-rep</code> \u7ed3\u679c</p> <p>\u53ef\u4ee5\u8f6c\u4e3a csv \u6587\u4ef6:</p> <p><code>ncu --import profile_output.ncu-rep --csv --page raw &gt; profile_output.csv</code></p> <p>\u770b\u6765\u4fe1\u606f\u5e94\u8be5\u975e\u5e38\u5168</p> <p><code>ncu --import profile_output.ncu-rep --page details</code></p> <p><code>ncu --import profile_output.ncu-rep --export profile_output.html</code></p>"},{"location":"daily/0328/","title":"0328","text":""},{"location":"daily/0328/#vae-3d-conv","title":"vae-3d conv","text":"<p>Wan-VAE \u5206\u5757\u56e0\u679c3D\u5377\u79ef</p> <p>from https://zhuanlan.zhihu.com/p/29268015945</p> <pre><code>class CausalConv3d(nn.Conv3d):\n    \"\"\"\n    Causal 3d convolusion.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._padding = (self.padding[2], self.padding[2], self.padding[1],\n                         self.padding[1], 2 * self.padding[0], 0)\n        self.padding = (0, 0, 0)\n\n    def forward(self, x, cache_x=None):\n        padding = list(self._padding)\n        if cache_x is not None and self._padding[4] &gt; 0:\n            cache_x = cache_x.to(x.device)\n            x = torch.cat([cache_x, x], dim=2)\n            padding[4] -= cache_x.shape[2]\n        x = F.pad(x, padding)\n\n        return super().forward(x)\n</code></pre> <pre><code>(Pdb) p self.decoder\nWanDecoder3d(\n  (nonlinearity): SiLU()\n  (conv_in): WanCausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n  (mid_block): WanMidBlock(\n    (attentions): ModuleList(\n      (0): WanAttentionBlock(\n        (norm): WanRMS_norm()\n        (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n        (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x WanResidualBlock(\n        (nonlinearity): SiLU()\n        (norm1): WanRMS_norm()\n        (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n        (norm2): WanRMS_norm()\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n        (conv_shortcut): Identity()\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): WanUpBlock(\n      (resnets): ModuleList(\n        (0-1): 2 x WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): WanResample(\n          (resample): Sequential(\n            (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n            (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n          (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n        )\n      )\n    )\n    (1): WanUpBlock(\n      (resnets): ModuleList(\n        (0): WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): WanCausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n        )\n        (1): WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): WanResample(\n          (resample): Sequential(\n            (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n            (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n          (time_conv): WanCausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n        )\n      )\n    )\n    (2): WanUpBlock(\n      (resnets): ModuleList(\n        (0-1): 2 x WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): WanResample(\n          (resample): Sequential(\n            (0): WanUpsample(scale_factor=(2.0, 2.0), mode='nearest-exact')\n            (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n    )\n    (3): WanUpBlock(\n      (resnets): ModuleList(\n        (0-1): 2 x WanResidualBlock(\n          (nonlinearity): SiLU()\n          (norm1): WanRMS_norm()\n          (conv1): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (norm2): WanRMS_norm()\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): WanCausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n          (conv_shortcut): Identity()\n        )\n      )\n    )\n  )\n  (norm_out): WanRMS_norm()\n  (conv_out): WanCausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n)\n</code></pre> <p>3D\u56e0\u679cVAE\u67b6\u6784</p> <ol> <li>cache\u5377\u79ef\u8f93\u5165\uff0c\u6d41\u5f0f\u63a8\u7406\uff0c\u6bcf\u6b21\u63a8\u7406\u4e00\u6bb5\uff0c\u663e\u5b58\u5360\u7528\u663e\u8457\u51cf\u5c11</li> <li><code>Wan-VAE</code>\u53ef\u4ee5\u7f16\u7801\u548c\u89e3\u7801\u4e0d\u9650\u5236\u957f\u5ea6\u76841080P\u89c6\u9891\uff0c\u800c\u4e0d\u4f1a\u4e22\u5931\u5386\u53f2\u65f6\u95f4\u4fe1\u606f\uff0c\u4f7f\u5176\u7279\u522b\u9002\u5408\u89c6\u9891\u751f\u6210\u4efb\u52a1</li> </ol> <p></p> <pre><code>class WanCausalConv3d(nn.Conv3d):\n    r\"\"\"\n    A custom 3D causal convolution layer with feature caching support.\n\n    This layer extends the standard Conv3D layer by ensuring causality in the time dimension and handling feature\n    caching for efficient inference.\n\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): Zero-padding added to all three sides of the input. Default: 0\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Union[int, Tuple[int, int, int]],\n        stride: Union[int, Tuple[int, int, int]] = 1,\n        padding: Union[int, Tuple[int, int, int]] = 0,\n    ) -&gt; None:\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n        )\n\n        # Set up causal padding\n        self._padding = (self.padding[2], self.padding[2], self.padding[1], self.padding[1], 2 * self.padding[0], 0)\n        self.padding = (0, 0, 0)\n\n    def forward(self, x, cache_x=None):\n        padding = list(self._padding)\n        if cache_x is not None and self._padding[4] &gt; 0:\n            cache_x = cache_x.to(x.device)\n            x = torch.cat([cache_x, x], dim=2)\n            padding[4] -= cache_x.shape[2]\n        x = F.pad(x, padding)\n        return super().forward(x)\n</code></pre> <pre><code>class WanMidBlock(nn.Module):\n\n    def __init__(self, dim: int, dropout: float = 0.0, non_linearity: str = \"silu\", num_layers: int = 1):\n        super().__init__()\n        self.dim = dim\n\n        # Create the components\n        resnets = [WanResidualBlock(dim, dim, dropout, non_linearity)]\n        attentions = []\n        for _ in range(num_layers):\n            attentions.append(WanAttentionBlock(dim))\n            resnets.append(WanResidualBlock(dim, dim, dropout, non_linearity))\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n        self.gradient_checkpointing = False\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        # First residual block\n        x = self.resnets[0](x, feat_cache, feat_idx)\n\n        # Process through attention and residual blocks\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            if attn is not None:\n                x = attn(x)\n\n            x = resnet(x, feat_cache, feat_idx)\n\n        return x\n</code></pre> <pre><code>class WanResidualBlock(nn.Module):\n    r\"\"\"\n    A custom residual block module.\n\n    Args:\n        in_dim (int): Number of input channels.\n        out_dim (int): Number of output channels.\n        dropout (float, optional): Dropout rate for the dropout layer. Default is 0.0.\n        non_linearity (str, optional): Type of non-linearity to use. Default is \"silu\".\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim: int,\n        out_dim: int,\n        dropout: float = 0.0,\n        non_linearity: str = \"silu\",\n    ) -&gt; None:\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.nonlinearity = get_activation(non_linearity)\n\n        # layers\n        self.norm1 = WanRMS_norm(in_dim, images=False)\n        self.conv1 = WanCausalConv3d(in_dim, out_dim, 3, padding=1)\n        self.norm2 = WanRMS_norm(out_dim, images=False)\n        self.dropout = nn.Dropout(dropout)\n        self.conv2 = WanCausalConv3d(out_dim, out_dim, 3, padding=1)\n        self.conv_shortcut = WanCausalConv3d(in_dim, out_dim, 1) if in_dim != out_dim else nn.Identity()\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        # Apply shortcut connection\n        h = self.conv_shortcut(x)\n\n        # First normalization and activation\n        x = self.norm1(x)\n        x = self.nonlinearity(x)\n\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n\n            x = self.conv1(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv1(x)\n\n        # Second normalization and activation\n        x = self.norm2(x)\n        x = self.nonlinearity(x)\n\n        # Dropout\n        x = self.dropout(x)\n\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n\n            x = self.conv2(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv2(x)\n\n        # Add residual connection\n        return x + h\n</code></pre> <pre><code>    def _decode(self, z: torch.Tensor, return_dict: bool = True) -&gt; Union[DecoderOutput, torch.Tensor]:\n        self.clear_cache()\n\n        iter_ = z.shape[2]\n        x = self.post_quant_conv(z)\n        breakpoint()\n        for i in range(iter_):\n            self._conv_idx = [0]\n            if i == 0:\n                # \u9010\u5e27\u53bb\u505a\u7684\u5904\u7406\n                out  = self.decoder(x[:, :, i : i + 1, :, :], feat_cache=self._feat_map, feat_idx=self._conv_idx)\n            else:\n                out_ = self.decoder(x[:, :, i : i + 1, :, :], feat_cache=self._feat_map, feat_idx=self._conv_idx)\n                out = torch.cat([out, out_], 2)\n\n        out = torch.clamp(out, min=-1.0, max=1.0)\n        self.clear_cache()\n        if not return_dict:\n            return (out,)\n\n        return DecoderOutput(sample=out)\n</code></pre> <pre><code>class WanDecoder3d(nn.Module):\n    def __init__(\n        self,\n        dim=128,\n        z_dim=4,\n        dim_mult=[1, 2, 4, 4],\n        num_res_blocks=2,\n        attn_scales=[],\n        temperal_upsample=[False, True, True],\n        dropout=0.0,\n        non_linearity: str = \"silu\",\n    ):\n        super().__init__()\n        self.dim = dim\n        self.z_dim = z_dim\n        self.dim_mult = dim_mult\n        self.num_res_blocks = num_res_blocks\n        self.attn_scales = attn_scales\n        self.temperal_upsample = temperal_upsample\n\n        self.nonlinearity = get_activation(non_linearity)\n\n        # dimensions\n        dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n        scale = 1.0 / 2 ** (len(dim_mult) - 2)\n\n        # init block\n        self.conv_in = WanCausalConv3d(z_dim, dims[0], 3, padding=1)\n\n        # middle blocks\n        self.mid_block = WanMidBlock(dims[0], dropout, non_linearity, num_layers=1)\n\n        # upsample blocks\n        self.up_blocks = nn.ModuleList([])\n        for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n            # residual (+attention) blocks\n            if i &gt; 0:\n                in_dim = in_dim // 2\n\n            # Determine if we need upsampling\n            upsample_mode = None\n            if i != len(dim_mult) - 1:\n                upsample_mode = \"upsample3d\" if temperal_upsample[i] else \"upsample2d\"\n\n            # Create and add the upsampling block\n            up_block = WanUpBlock(\n                in_dim=in_dim,\n                out_dim=out_dim,\n                num_res_blocks=num_res_blocks,\n                dropout=dropout,\n                upsample_mode=upsample_mode,\n                non_linearity=non_linearity,\n            )\n            self.up_blocks.append(up_block)\n\n            # Update scale for next iteration\n            if upsample_mode is not None:\n                scale *= 2.0\n\n        # output blocks\n        self.norm_out = WanRMS_norm(out_dim, images=False)\n        self.conv_out = WanCausalConv3d(out_dim, 3, 3, padding=1)\n\n        self.gradient_checkpointing = False\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        ## conv1\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                # cache last frame of last two chunk\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n            x = self.conv_in(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv_in(x)\n\n        ## middle\n        x = self.mid_block(x, feat_cache, feat_idx)\n\n        ## upsamples\n        for up_block in self.up_blocks:\n            x = up_block(x, feat_cache, feat_idx)\n\n        ## head\n        x = self.norm_out(x)\n        x = self.nonlinearity(x)\n        if feat_cache is not None:\n            idx = feat_idx[0]\n            cache_x = x[:, :, -CACHE_T:, :, :].clone()\n            if cache_x.shape[2] &lt; 2 and feat_cache[idx] is not None:\n                # cache last frame of last two chunk\n                cache_x = torch.cat([feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device), cache_x], dim=2)\n            x = self.conv_out(x, feat_cache[idx])\n            feat_cache[idx] = cache_x\n            feat_idx[0] += 1\n        else:\n            x = self.conv_out(x)\n        return x\n</code></pre> <pre><code>graph LR\n    subgraph \"\u65f6\u95f4\u7ef4\u5ea6\"\n        T1[t-2] --&gt; T2[t-1] --&gt; T3[t]\n    end\n\n    subgraph \"\u56e0\u679c\u5377\u79ef\"\n        T1 --&gt; O[\u8f93\u51fat]\n        T2 --&gt; O\n        T3 --&gt; O\n        style O fill:#f96\n    end\n</code></pre> <pre><code>class WanUpBlock(nn.Module):\n    def __init__(\n        self,\n        in_dim: int,\n        out_dim: int,\n        num_res_blocks: int,\n        dropout: float = 0.0,\n        upsample_mode: Optional[str] = None,\n        non_linearity: str = \"silu\",\n    ):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n\n        # Create layers list\n        resnets = []\n        # Add residual blocks and attention if needed\n        current_dim = in_dim\n        for _ in range(num_res_blocks + 1):\n            resnets.append(WanResidualBlock(current_dim, out_dim, dropout, non_linearity))\n            current_dim = out_dim\n\n        self.resnets = nn.ModuleList(resnets)\n\n        # Add upsampling layer if needed\n        self.upsamplers = None\n        if upsample_mode is not None:\n            self.upsamplers = nn.ModuleList([WanResample(out_dim, mode=upsample_mode)])\n\n        self.gradient_checkpointing = False\n\n    def forward(self, x, feat_cache=None, feat_idx=[0]):\n        for resnet in self.resnets:\n            if feat_cache is not None:\n                x = resnet(x, feat_cache, feat_idx)\n            else:\n                x = resnet(x)\n\n        if self.upsamplers is not None:\n            if feat_cache is not None:\n                x = self.upsamplers[0](x, feat_cache, feat_idx)\n            else:\n                x = self.upsamplers[0](x)\n        return x\n</code></pre> <p>\u73b0\u5728\u6700\u5927\u7684\u95ee\u9898\u5c31\u662f <code>feat_cache</code></p> <p>\u8c8c\u4f3c\u6bcf\u4e00\u8f6e\u90fd\u4f1a\u6709\u4e00\u4e2acache\u8fc7\u7a0b\uff1f\u800c\u4e14\u5728\u4e00\u4e2a self.decoder \u5c31\u4f1a\u51e0\u4e4e\u6253\u6ee125\u4e2afeat_cache\u4f4d\u7f6e\u3002</p> <p>\u8fd9\u91cc\u7684\u52a8\u6001\u5f88\u591a.\u5177\u4f53\u5185\u5bb9\u5305\u62ec.</p> <p>\u4ed6\u7684shape\u4e5f\u5f88\u5947\u602a.</p> <p>paper: https://files.alicdn.com/tpsservice/5c9de1c74de03972b7aa657e5a54756b.pdf</p> <p></p> <p></p> <p>\u6574\u4f53\u7ed3\u6784\u53c2\u8003\uff1a</p> <p></p> <p>\u7b2c\u4e8c\u538b\u7f29\u5e27\u7684\u5185\u5bb9 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 1, 0]\n</code></pre> \u7b2c\u4e09\u538b\u7f29\u5e27 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\n</code></pre></p> <p>\u7b2c4\u5e27 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\n</code></pre></p> <p>5 <pre><code>x.shape:  torch.Size([1, 16, 1, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nfeat_cache torch.Size([1, 16, 2, 90, 128])\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [0, 0, 0, 0, 2, 0]\nafter   [0, 0, 0, 0, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\nbefore  [1, 1, 1, 1, 2, 0]\nafter   [1, 1, 1, 1, 0, 0]\n</code></pre></p> <p>\u76ee\u524d\u53d1\u73b0 \u7b2c\u4e00\u5e27\u548c\u7b2c\u4e8c\u5e27\u7684\u884c\u4e3a\u4e0e\u540e\u9762\u90fd\u4e0d\u592a\u4e00\u6837</p>"},{"location":"daily/0328/#_1","title":"\u65b0\u7684\u7b56\u7565","text":"<ol> <li>\u5f97\u5230\u6240\u6709\u7684shape \u8d70\u52a8\u6001</li> </ol> <p>\u7528 torchview \u5f97\u5230\u6240\u6709\u7684\u7ed3\u6784\u548cshape</p>"},{"location":"train/unsloth/","title":"Unsloth","text":"<p>\u6211\u611f\u89c9\u5f97\u5148\u770b\u770b\u6027\u80fd\uff0c\u7136\u540e\u518d\u60f3\u7740\u4ee5\u540e\u4f18\u5316\u3002 <pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Supports Llama, Mistral - replace this!\n    max_seq_length = 2048, # Supports RoPE Scaling internally, so choose any!\n    load_in_4bit = True,\n)\n</code></pre></p>"}]}