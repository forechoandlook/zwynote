
## fastvideo 学习

主要克服 计算。

1. 蒸馏 distillation
2. sliding tile attention and TeaCache(Timestep Embedding Aware Cache (TeaCache))
3. parallelism strategies and memory optimization
4. quantization


## sliding tile attention 

5秒的720p视频片段 ---> 115K tokens 

视频的shape是`（L，L, L)`，即使L稍微增加一点，token数量也会呈立方级爆炸式增长

![](https://pic2.zhimg.com/v2-168c1f0036ab820bebcbb8735fed473d_1440w.jpg)

作者的分析非常独到

3D sliding window attention (SWA）？

mask计算能超过计算block本身的成本，flexattention论文提到计算一个简单的causal attention mask会增加15%的latency.

加速方法：

1. video : (L,L,L) ===> tile (T, T, T) with attention block(B,B) B=T*3
2. 把3D的 (L,L,L) 转为1D seq 输入给 attention kernel的时候，一个tile内的token 会有连续的index。并且STA的window side 也需要是（T，T, T）的整数倍。
3. 注意力窗口以（T，T, T) 为单位逐块移动。对于每个local window，中心的query tile (不是query token), 会attend整个window的KV.



## framepack搭建


## 如何快速生成视频？


